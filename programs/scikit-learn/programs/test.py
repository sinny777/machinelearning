import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn import metrics

#Loading the data set - training data.
df = pd.read_csv('intents_data.csv', sep=',')

X = df['utterances']
y = df['intent']

# X_train, X_test, Y_train, Y_test = train_test_split(X, y, random_state = 0, test_size=0.05)

# print('X_train.shape: >> ', X_train.shape)
# print('Y_train.shape: >> ', Y_train.shape)
# print('X_test.shape: >> ', X_test.shape)
# print('Y_test.shape: >> ', Y_test.shape)

from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(X)
# print(X_train_counts.shape)

from sklearn.feature_extraction.text import TfidfTransformer
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
# print(X_train_tfidf.shape)

# Machine Learning
# Training Naive Bayes (NB) classifier on training data.
from sklearn.naive_bayes import MultinomialNB
clf = MultinomialNB().fit(X_train_tfidf, y)

# Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:
# The names â€˜vectâ€™ , â€˜tfidfâ€™ and â€˜clfâ€™ are arbitrary but will be used later.
# We will be using the 'text_clf' going forward.
from sklearn.pipeline import Pipeline
text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])
text_clf = text_clf.fit(X, y)

test_docs = pd.Series(["How is the weather outside", "tell me your favourite movie", "I want to hear music"])

# print("Transformed: >> ", count_vect.transform(test_docs))
print("Prediction: >> ", clf.predict(count_vect.transform(test_docs)))
print("Prediction: >> ", text_clf.predict(test_docs))

true_prediction = pd.Series(["weather", "about_VA", "turn_on"])

# Performance of NB Classifier
import numpy as np
predicted = text_clf.predict(test_docs)
print(np.mean(predicted == true_prediction))

# Training Support Vector Machines - SVM and calculating its performance

from sklearn.linear_model import SGDClassifier
text_clf_svm = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),
                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=5, random_state=42))])

text_clf_svm = text_clf_svm.fit(X, y)
predicted_svm = text_clf_svm.predict(test_docs)
print(predicted_svm)
print(np.mean(predicted_svm == true_prediction))

# In[18]:

# Grid Search
# Here, we are creating a list of parameters for which we would like to do performance tuning.
# All the parameters name start with the classifier name (remember the arbitrary name we gave).
# E.g. vect__ngram_range; here we are telling to use unigram and bigrams and choose the one which is optimal.

from sklearn.model_selection import GridSearchCV
parameters = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'clf__alpha': (1e-2, 1e-3)}


# In[19]:

# Next, we create an instance of the grid search by passing the classifier, parameters
# and n_jobs=-1 which tells to use multiple cores from user machine.

gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)
gs_clf = gs_clf.fit(X, y)


# In[23]:

# To see the best mean score and the params, run the following code

gs_clf.best_score_
gs_clf.best_params_

# Output for above should be: The accuracy has now increased to ~90.6% for the NB classifier (not so naive anymore! ðŸ˜„)
# and the corresponding parameters are {â€˜clf__alphaâ€™: 0.01, â€˜tfidf__use_idfâ€™: True, â€˜vect__ngram_rangeâ€™: (1, 2)}.


# In[24]:

# Similarly doing grid search for SVM
from sklearn.model_selection import GridSearchCV
parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False),'clf-svm__alpha': (1e-2, 1e-3)}

gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)
gs_clf_svm = gs_clf_svm.fit(X, y)


gs_clf_svm.best_score_
gs_clf_svm.best_params_


# In[25]:

# NLTK
# Removing stop words
from sklearn.pipeline import Pipeline
text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()),
                     ('clf', MultinomialNB())])


# In[26]:

# Stemming Code

import nltk
nltk.download()

from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer("english", ignore_stopwords=True)

class StemmedCountVectorizer(CountVectorizer):
    def build_analyzer(self):
        analyzer = super(StemmedCountVectorizer, self).build_analyzer()
        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])

stemmed_count_vect = StemmedCountVectorizer(stop_words='english')

text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect), ('tfidf', TfidfTransformer()),
                             ('mnb', MultinomialNB(fit_prior=False))])

text_mnb_stemmed = text_mnb_stemmed.fit(X, y)

predicted_mnb_stemmed = text_mnb_stemmed.predict(test_docs)
print('predicted_mnb_stemmed: >> ', predicted_mnb_stemmed)
print(np.mean(predicted_mnb_stemmed == true_prediction))


# In[ ]:
