{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "#### Useful Links: \n  * [Tensorflow to predict handwritten digits](https://dataplatform.cloud.ibm.com/exchange/public/entry/view/b5eac6e919cd6fbcc5824de04a00ec65)\n  * [Keras experiment with HPO to predict digits](https://dataplatform.cloud.ibm.com/analytics/notebooks/v2/1c9801fc-5063-4564-a756-75e99be47cd0/view?access_token=d38aa735e323be34260be5fcf65813cea1f5f8a17a256e1d2f23796fdcd11a7d)\n  * [Watson Nao Robot Notebook](https://github.com/IBM/watson-nao-robot/blob/master/Notebook/Robo_Notebook.ipynb)\n  * [Watson Document Co-Relation](https://github.com/IBM/watson-document-co-relation)\n  * [Cloud Object Storage](https://console.bluemix.net/docs/services/cloud-object-storage/libraries/python.html#using-python)\n  * [Watson Machine Learning Client Documentation](https://wml-api-pyclient.mybluemix.net/)\n    ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "1.3.0\n"
                }
            ], 
            "source": "import tensorflow as tf\nprint(tf.__version__)"
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Requirement not upgraded as not directly required: keras in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages\nRequirement not upgraded as not directly required: six>=1.9.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from keras)\nRequirement not upgraded as not directly required: pyyaml in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from keras)\nRequirement not upgraded as not directly required: scipy>=0.14 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from keras)\nRequirement not upgraded as not directly required: numpy>=1.9.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from keras)\nRequirement already up-to-date: ibm-cos-sdk in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages\nRequirement not upgraded as not directly required: ibm-cos-sdk-core==2.*,>=2.0.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk)\nRequirement not upgraded as not directly required: ibm-cos-sdk-s3transfer==2.*,>=2.0.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk)\nRequirement not upgraded as not directly required: python-dateutil<3.0.0,>=2.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk)\nRequirement not upgraded as not directly required: docutils>=0.10 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk)\nRequirement not upgraded as not directly required: jmespath<1.0.0,>=0.7.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk)\nRequirement not upgraded as not directly required: six>=1.5 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from python-dateutil<3.0.0,>=2.1->ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk)\nRequirement already up-to-date: nltk in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages\nRequirement not upgraded as not directly required: six in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from nltk)\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/dsxuser/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /home/dsxuser/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/dsxuser/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package maxent_ne_chunker to\n[nltk_data]     /home/dsxuser/nltk_data...\n[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n[nltk_data] Downloading package words to /home/dsxuser/nltk_data...\n[nltk_data]   Package words is already up-to-date!\nRequirement already up-to-date: socketIO_client_nexus in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages\nRequirement not upgraded as not directly required: websocket-client in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from socketIO_client_nexus)\nRequirement not upgraded as not directly required: requests>=2.7.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from socketIO_client_nexus)\nRequirement not upgraded as not directly required: six in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from socketIO_client_nexus)\nRequirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests>=2.7.0->socketIO_client_nexus)\nRequirement not upgraded as not directly required: idna<2.7,>=2.5 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests>=2.7.0->socketIO_client_nexus)\nRequirement not upgraded as not directly required: urllib3<1.23,>=1.21.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests>=2.7.0->socketIO_client_nexus)\nRequirement not upgraded as not directly required: certifi>=2017.4.17 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests>=2.7.0->socketIO_client_nexus)\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "Using TensorFlow backend.\n"
                }
            ], 
            "source": "\n# INSTALL DEPENDENCIES\n# if(tf.__version__ == '1.9.0'):\n#     print(tf.__version__)\n# else:\n#     !pip install --upgrade tensorflow\n#     print(tf.__version__)\n\n!pip install keras\n!pip install -U ibm-cos-sdk\n!pip install --upgrade nltk\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('maxent_ne_chunker')\nnltk.download('words')\n\n!pip install -U socketIO_client_nexus\n  \nimport pandas as pd\nimport numpy as np\nimport random\n\nimport os.path\nfrom os import path\n\nfrom io import  StringIO\nimport requests\nimport json\nfrom datetime import datetime\nimport time\n\n# things we need for NLP\nimport nltk\nfrom nltk.cluster.util import cosine_distance\nfrom nltk import word_tokenize,sent_tokenize,ne_chunk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.lancaster import LancasterStemmer\nstemmer = LancasterStemmer()\n\nimport keras\n\nimport sys\nimport types\nfrom botocore.client import Config\nimport ibm_boto3"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "mkdir: cannot create directory \u2018results\u2019: File exists\r\n"
                }
            ], 
            "source": "REMOTE = True\nMODEL_PATH = \"results/nlc_car_model.h5\"\nMODEL_WEIGHTS_PATH = \"results/nlc_car_weights.hdf5\"\n!mkdir results"
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n# You might want to remove those credentials before you share your notebook.\ndef update_configuration(conf):\n    global config\n    config = conf\n    print(config[\"cos_credentials\"])\n    print(config[\"cos_data\"])\n"
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def multi_part_upload(bucket_name, item_name, file_path):\n    try:\n        print(\"Starting file transfer for {0} to bucket: {1}\\n\".format(item_name, bucket_name))\n        cos = ibm_boto3.resource(service_name='s3',\n            ibm_api_key_id=config[\"cos_credentials\"]['IBM_API_KEY_ID'],\n            ibm_auth_endpoint=config[\"cos_credentials\"]['IBM_AUTH_ENDPOINT'],\n            config=Config(signature_version='oauth'),\n            endpoint_url=config[\"cos_credentials\"]['ENDPOINT'])\n        # set 5 MB chunks\n        part_size = 1024 * 1024 * 5\n\n        # set threadhold to 15 MB\n        file_threshold = 1024 * 1024 * 15\n\n        # set the transfer threshold and chunk size\n        transfer_config = ibm_boto3.s3.transfer.TransferConfig(\n            multipart_threshold=file_threshold,\n            multipart_chunksize=part_size\n        )\n\n        # the upload_fileobj method will automatically execute a multi-part upload \n        # in 5 MB chunks for all files over 15 MB\n        with open(file_path, \"rb\") as file_data:\n            cos.Object(bucket_name, item_name).upload_fileobj(\n                Fileobj=file_data,\n                Config=transfer_config\n            )\n\n        print(\"Transfer for {0} Complete!\\n\".format(item_name))\n    except Exception as e:\n        print(\"Unable to complete multi-part upload: {0}\".format(e))\n"
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def get_object_cos(bucket_name, item_name, path_to_download):\n    try:\n        print(\"Fetching file {0} from bucket: {1}\\n\".format(item_name, bucket_name))\n        cos = ibm_boto3.resource(service_name='s3',\n            ibm_api_key_id=config[\"cos_credentials\"]['IBM_API_KEY_ID'],\n            ibm_auth_endpoint=config[\"cos_credentials\"]['IBM_AUTH_ENDPOINT'],\n            config=Config(signature_version='oauth'),\n            endpoint_url=config[\"cos_credentials\"]['ENDPOINT'])\n        \n        cos.Object(bucket_name, item_name).download_file(path_to_download)\n\n        print(\"Download for {0} Complete!\\n\".format(item_name))\n    except Exception as e:\n        print(\"Unable to download file: {0}\".format(e))\n"
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# LOAD DATA\ndef load_data():\n    global df\n    global cos\n    def __iter__(self): return 0\n\n    # The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n    cos = ibm_boto3.client(service_name='s3',\n        ibm_api_key_id=config[\"cos_credentials\"]['IBM_API_KEY_ID'],\n        ibm_auth_endpoint=config[\"cos_credentials\"]['IBM_AUTH_ENDPOINT'],\n        config=Config(signature_version='oauth'),\n        endpoint_url=config[\"cos_credentials\"]['ENDPOINT'])\n\n    body = cos.get_object(Bucket=config[\"cos_data\"]['BUCKET'],Key=config[\"cos_data\"]['FILE'])['Body']\n    # add missing __iter__ method, so pandas accepts body as file-like object\n    if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\n    df = pd.read_csv(body)\n    df.head()   \n"
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def prepare_documents():\n    global classes\n    global documents\n    global words\n    classes = []\n    documents = []\n    words = []\n    ignore_words = ['?']    \n    \n    # loop through each sentence in our intents patterns\n    for i in range(len(df)):\n        # tokenize each word in the sentence\n        w = nltk.word_tokenize(df[\"utterances\"][i])\n        # add to our words list\n        words.extend(w)\n        # add to documents in our corpus\n        documents.append((w, df[\"intent\"][i]))\n        # add to our classes list\n        if df[\"intent\"][i] not in classes:\n            classes.append(df[\"intent\"][i])\n\n    # stem and lower each word and remove duplicates\n    words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n    words = sorted(list(set(words)))\n\n    # remove duplicates\n    classes = sorted(list(set(classes)))\n\n    print (len(documents), \"documents\")\n    print (len(classes), \"classes\", classes)\n    # print (len(words), \"unique stemmed words\", words)\n"
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# create our training data\ndef prepare_for_training():\n    training = []\n    output = []\n    global train_x\n    global train_y\n    # create an empty array for our output\n    output_empty = [0] * len(classes)\n    # training set, bag of words for each sentence\n    for doc in documents:\n        # initialize our bag of words\n        bag = []\n        # list of tokenized words for the pattern\n        pattern_words = doc[0]\n        # stem each word\n        pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n        # create our bag of words array\n        for w in words:\n            bag.append(1) if w in pattern_words else bag.append(0)\n\n        # output is a '0' for each tag and '1' for current tag\n        output_row = list(output_empty)\n        output_row[classes.index(doc[1])] = 1\n\n        training.append([bag, output_row])\n        \n    # shuffle our features and turn into np.array\n    random.shuffle(training)\n    training = np.array(training)\n    \n    # create train and test lists\n    train_x = list(training[:,0])\n    train_y = list(training[:,1])\n"
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from keras.models import Sequential\nfrom keras.layers import Dense, Input, concatenate, Activation\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n# from tf.keras.layers.pooling import GlobalMaxPooling1D, MaxPooling1D\n# from tf.keras.layers.core import Dropout\nfrom keras import backend as K\n\n# CREATE ML MODEL\ndef create_model():\n    K.clear_session()\n    # tf.global_variables_initializer()\n    tf.reset_default_graph()\n    global model\n    model = Sequential()\n    # model.add(Dense(output_dim=8,init ='uniform',activation='relu', input_dim=len(train_x[0])))\n    model.add(Dense(8, activation='relu', input_shape=(np.asarray(train_x[0]).shape)))\n    # model.add(Dense(8, activation='relu', input_dim=(len(train_x))))\n    # model.add(Activation('relu'))\n    # model.add(Dropout(0.3))\n    model.add(Dense(8, activation='relu'))\n    # model.add(Activation('relu'))\n    # model.add(Dropout(0.3))\n    model.add(Dense(8, activation='relu'))\n    # model.add(Activation('relu'))\n    # model.add(Dropout(0.3))\n    model.add(Dense(np.asarray(train_y[0]).shape[0], activation='softmax'))\n    model.summary()\n    \n    tbCallBack = keras.callbacks.TensorBoard(log_dir='keras_logs', write_graph=True)\n    \n    # model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=0, mode='auto')\n    checkpointer = ModelCheckpoint(filepath=MODEL_WEIGHTS_PATH, verbose=0, save_best_only=True) # Save best model\n    model.fit(np.asarray(train_x), np.asarray(train_y), epochs=200, batch_size=8,  verbose=1, validation_split=0.1, callbacks=[tbCallBack, monitor, checkpointer])\n    model.load_weights(MODEL_WEIGHTS_PATH) # load weights from best model\n    scores = model.evaluate(np.asarray(train_x), np.asarray(train_y))\n    print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n    model.save(MODEL_PATH)\n    print(\"<<<<<<<< ML MODEL CREATED AND SAVED >>>>>>>>>>>\\n\\n\")"
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def save_model_COS():\n    if(REMOTE):\n        multi_part_upload(config[\"cos_data\"]['BUCKET'], MODEL_PATH, MODEL_PATH)\n        multi_part_upload(config[\"cos_data\"]['BUCKET'], MODEL_WEIGHTS_PATH, MODEL_WEIGHTS_PATH)\n    "
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def reset_all():\n    update_configuration(conf)\n    load_data()\n    prepare_documents()\n    prepare_for_training()\n    create_model()\n    save_model_COS()\n"
        }, 
        {
            "source": "# Code to Classify text using the ML Model created", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def fetch_ml_model_cos():\n    if(path.exists(MODEL_PATH) == False):\n        get_object_cos(config[\"cos_data\"]['BUCKET'], MODEL_PATH, MODEL_PATH)\n    if(path.exists(MODEL_WEIGHTS_PATH) == False):\n        get_object_cos(config[\"cos_data\"]['BUCKET'], MODEL_WEIGHTS_PATH, MODEL_WEIGHTS_PATH)\n"
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from keras.models import load_model\ndef load_ml_model():\n    global model\n    try:\n        model\n    except NameError:\n        print(\"<<< ML Model Needs to be loaded >>>>>\")\n        # load our saved model\n        fetch_ml_model_cos()\n        model = load_model(MODEL_PATH)        \n    else:\n        print(\"<<< ML Model Already Exists >>>>>\")        \n"
        }, 
        {
            "execution_count": 16, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def clean_up_sentence(sentence):\n    # tokenize the pattern\n    sentence_words = nltk.word_tokenize(sentence)\n    # stem each word\n    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n    return sentence_words\n\n# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\ndef bow(sentence, words, show_details=False):\n    # tokenize the pattern\n    sentence_words = clean_up_sentence(sentence)\n    # bag of words\n    bag = [0]*len(words)  \n    for s in sentence_words:\n        for i,w in enumerate(words):\n            if w == s: \n                bag[i] = 1\n                if show_details:\n                    print (\"found in bag: %s\" % w)\n\n    return(np.array(bag))"
        }, 
        {
            "execution_count": 17, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# create a data structure to hold user context\ncontext = {}\n\nERROR_THRESHOLD = 0.25\ndef classify(sentence):\n    # generate probabilities from the model\n    load_ml_model()\n    to_predict = bow(sentence, words)\n    if (to_predict.ndim == 1):\n        to_predict = np.array([to_predict])\n    \n    results = model.predict([to_predict])[0]\n    # filter out predictions below a threshold\n    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n    # sort by strength of probability\n    results.sort(key=lambda x: x[1], reverse=True)\n    return_list = []\n    for r in results:\n        return_list.append((classes[r[0]], r[1]))\n    # return tuple of intent and probability\n    return return_list"
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def reset_for_classification():\n    update_configuration(conf)\n    load_data()\n    prepare_documents()\n    prepare_for_training()"
        }, 
        {
            "execution_count": 19, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from socketIO_client_nexus import SocketIO, BaseNamespace, LoggingNamespace\n\ndef on_connect():\n    print('on_connect')\n\ndef on_disconnect():\n    print('on_disconnect')\n\ndef on_reconnect():\n    print('on_reconnect')\n\ndef on_response(*message):\n    msg = json.loads(json.dumps(message))\n    print(type(msg))\n    print('\\n\\non_response: >> ', msg[0])\n    command = msg[0][\"command\"]\n    params = msg[0][\"params\"]\n    if command == \"reset_all\":\n        reset_all()\n    elif command == \"classify\":\n        results = classify(params[\"text\"])\n        print(results)\n    else:\n        print(\"Command not recognized....\")\n\ndef connectSocketIO():\n#     SocketIO('https://localhost', verify=False)\n    with SocketIO('https://my-watson-assistant-api.mybluemix.net', verify=False) as socketIO:\n        # with SocketIO('localhost', verify=False) as socketIO:\n        socketIO.on('connect', on_connect)\n        socketIO.on('disconnect', on_disconnect)\n        socketIO.on('reconnect', on_reconnect)\n        socketIO.on('/ml', on_response)\n        socketIO.wait()\n"
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "{'IBM_API_KEY_ID': 'jDLQvkwwo3h77B5MWgqOTUq25D94Xr6CGrb_6dYmVcj-', 'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com', 'IBM_AUTH_ENDPOINT': 'https://iam.ng.bluemix.net/oidc/token'}\n{'FILE': 'raw_car_dashboard_ml.csv', 'BUCKET': 'myml-donotdelete-pr-zhsoop3fasxh7h'}\n2532 documents\n26 classes ['about_VA', 'capabilites', 'capabilities', 'compound_questions', 'decision_replies', 'goodbyes', 'greetings', 'improving_system', 'information_request', 'interface_interactions', 'interface_issues', 'locate_amenity', 'navigation', 'negative_reaction', 'not_specified', 'out_of_scope', 'phone', 'positive_reaction', 'selections', 'system_reliance', 'traffic_update', 'turn_down', 'turn_off', 'turn_on', 'turn_up', 'weather']\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 8)                 10656     \n_________________________________________________________________\ndense_2 (Dense)              (None, 8)                 72        \n_________________________________________________________________\ndense_3 (Dense)              (None, 8)                 72        \n_________________________________________________________________\ndense_4 (Dense)              (None, 26)                234       \n=================================================================\nTotal params: 11,034\nTrainable params: 11,034\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 2278 samples, validate on 254 samples\nEpoch 1/200\n2278/2278 [==============================] - 1s 457us/step - loss: 2.6355 - acc: 0.2959 - val_loss: 2.1120 - val_acc: 0.2795\nEpoch 2/200\n2278/2278 [==============================] - 1s 473us/step - loss: 2.0181 - acc: 0.3077 - val_loss: 1.8488 - val_acc: 0.3346\nEpoch 3/200\n2278/2278 [==============================] - 1s 479us/step - loss: 1.8698 - acc: 0.3218 - val_loss: 1.7206 - val_acc: 0.3346\nEpoch 4/200\n2278/2278 [==============================] - 1s 482us/step - loss: 1.7406 - acc: 0.4205 - val_loss: 1.5475 - val_acc: 0.5827\nEpoch 5/200\n2278/2278 [==============================] - 1s 511us/step - loss: 1.5347 - acc: 0.5891 - val_loss: 1.3005 - val_acc: 0.6575\nEpoch 6/200\n2278/2278 [==============================] - 1s 478us/step - loss: 1.3526 - acc: 0.6299 - val_loss: 1.1717 - val_acc: 0.6811\nEpoch 7/200\n2278/2278 [==============================] - 1s 483us/step - loss: 1.2480 - acc: 0.6572 - val_loss: 1.0934 - val_acc: 0.6929\nEpoch 8/200\n2278/2278 [==============================] - 1s 481us/step - loss: 1.1665 - acc: 0.7028 - val_loss: 1.0311 - val_acc: 0.6929\nEpoch 9/200\n2278/2278 [==============================] - 1s 483us/step - loss: 1.0979 - acc: 0.7173 - val_loss: 0.9794 - val_acc: 0.7008\nEpoch 10/200\n2278/2278 [==============================] - 1s 482us/step - loss: 1.0362 - acc: 0.7344 - val_loss: 0.9390 - val_acc: 0.7047\nEpoch 11/200\n2278/2278 [==============================] - 1s 483us/step - loss: 0.9878 - acc: 0.7463 - val_loss: 0.9108 - val_acc: 0.7165\nEpoch 12/200\n2278/2278 [==============================] - 1s 514us/step - loss: 0.9441 - acc: 0.7572 - val_loss: 0.8721 - val_acc: 0.7126\nEpoch 13/200\n2278/2278 [==============================] - 1s 483us/step - loss: 0.9019 - acc: 0.7665 - val_loss: 0.8431 - val_acc: 0.7402\nEpoch 14/200\n2278/2278 [==============================] - 1s 511us/step - loss: 0.8667 - acc: 0.7695 - val_loss: 0.8191 - val_acc: 0.7441\nEpoch 15/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.8315 - acc: 0.7752 - val_loss: 0.8039 - val_acc: 0.7480\nEpoch 16/200\n2278/2278 [==============================] - 1s 478us/step - loss: 0.8080 - acc: 0.7827 - val_loss: 0.7807 - val_acc: 0.7441\nEpoch 17/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.7839 - acc: 0.7845 - val_loss: 0.7731 - val_acc: 0.7480\nEpoch 18/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.7585 - acc: 0.7849 - val_loss: 0.7663 - val_acc: 0.7638\nEpoch 19/200\n2278/2278 [==============================] - 1s 519us/step - loss: 0.7356 - acc: 0.7963 - val_loss: 0.7620 - val_acc: 0.7598\nEpoch 20/200\n2278/2278 [==============================] - 1s 515us/step - loss: 0.7157 - acc: 0.8007 - val_loss: 0.7539 - val_acc: 0.7638\nEpoch 21/200\n2278/2278 [==============================] - 1s 482us/step - loss: 0.6950 - acc: 0.8068 - val_loss: 0.7578 - val_acc: 0.7598\nEpoch 22/200\n2278/2278 [==============================] - 1s 513us/step - loss: 0.6796 - acc: 0.8090 - val_loss: 0.7518 - val_acc: 0.7638\nEpoch 23/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.6623 - acc: 0.8174 - val_loss: 0.7422 - val_acc: 0.7677\nEpoch 24/200\n2278/2278 [==============================] - 1s 507us/step - loss: 0.6455 - acc: 0.8227 - val_loss: 0.7561 - val_acc: 0.7717\nEpoch 25/200\n2278/2278 [==============================] - 1s 487us/step - loss: 0.6293 - acc: 0.8292 - val_loss: 0.7310 - val_acc: 0.7913\nEpoch 26/200\n2278/2278 [==============================] - 1s 479us/step - loss: 0.6190 - acc: 0.8301 - val_loss: 0.7315 - val_acc: 0.7913\nEpoch 27/200\n2278/2278 [==============================] - 1s 488us/step - loss: 0.6009 - acc: 0.8385 - val_loss: 0.7339 - val_acc: 0.7874\nEpoch 28/200\n2278/2278 [==============================] - 1s 484us/step - loss: 0.5867 - acc: 0.8433 - val_loss: 0.7309 - val_acc: 0.7874\nEpoch 29/200\n2278/2278 [==============================] - 1s 484us/step - loss: 0.5728 - acc: 0.8428 - val_loss: 0.7253 - val_acc: 0.7913\nEpoch 30/200\n2278/2278 [==============================] - 1s 480us/step - loss: 0.5592 - acc: 0.8494 - val_loss: 0.7292 - val_acc: 0.7835\nEpoch 31/200\n2278/2278 [==============================] - 1s 486us/step - loss: 0.5475 - acc: 0.8516 - val_loss: 0.7354 - val_acc: 0.7953\nEpoch 32/200\n2278/2278 [==============================] - 1s 484us/step - loss: 0.5355 - acc: 0.8582 - val_loss: 0.7244 - val_acc: 0.8031\nEpoch 33/200\n2278/2278 [==============================] - 1s 484us/step - loss: 0.5281 - acc: 0.8635 - val_loss: 0.7086 - val_acc: 0.8110\nEpoch 34/200\n2278/2278 [==============================] - 1s 488us/step - loss: 0.5140 - acc: 0.8670 - val_loss: 0.7161 - val_acc: 0.8110\nEpoch 35/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.5034 - acc: 0.8687 - val_loss: 0.7148 - val_acc: 0.8031\nEpoch 36/200\n2278/2278 [==============================] - 1s 510us/step - loss: 0.4991 - acc: 0.8718 - val_loss: 0.7023 - val_acc: 0.8110\nEpoch 37/200\n2278/2278 [==============================] - 1s 477us/step - loss: 0.4849 - acc: 0.8771 - val_loss: 0.7268 - val_acc: 0.7992\nEpoch 38/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.4774 - acc: 0.8784 - val_loss: 0.7070 - val_acc: 0.8071\nEpoch 39/200\n2278/2278 [==============================] - 1s 483us/step - loss: 0.4685 - acc: 0.8819 - val_loss: 0.7079 - val_acc: 0.8071\nEpoch 40/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.4621 - acc: 0.8867 - val_loss: 0.7090 - val_acc: 0.8110\nEpoch 41/200\n2278/2278 [==============================] - 1s 521us/step - loss: 0.4534 - acc: 0.8859 - val_loss: 0.7030 - val_acc: 0.8110\n2532/2532 [==============================] - 0s 38us/step\n\nacc: 87.16%\n<<<<<<<< ML MODEL CREATED AND SAVED >>>>>>>>>>>\n\n\nStarting file transfer for results/nlc_car_model.h5 to bucket: myml-donotdelete-pr-zhsoop3fasxh7h\n\nTransfer for results/nlc_car_model.h5 Complete!\n\n"
                }
            ], 
            "source": "reset_all()"
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "{'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com', 'IBM_API_KEY_ID': 'jDLQvkwwo3h77B5MWgqOTUq25D94Xr6CGrb_6dYmVcj-', 'IBM_AUTH_ENDPOINT': 'https://iam.ng.bluemix.net/oidc/token'}\n{'FILE': 'raw_car_dashboard_ml.csv', 'BUCKET': 'myml-donotdelete-pr-zhsoop3fasxh7h'}\n2532 documents\n26 classes ['about_VA', 'capabilites', 'capabilities', 'compound_questions', 'decision_replies', 'goodbyes', 'greetings', 'improving_system', 'information_request', 'interface_interactions', 'interface_issues', 'locate_amenity', 'navigation', 'negative_reaction', 'not_specified', 'out_of_scope', 'phone', 'positive_reaction', 'selections', 'system_reliance', 'traffic_update', 'turn_down', 'turn_off', 'turn_on', 'turn_up', 'weather']\n"
                }
            ], 
            "source": "reset_for_classification()"
        }, 
        {
            "execution_count": 24, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "<<< ML Model Already Exists >>>>>\n"
                }, 
                {
                    "execution_count": 24, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "classify('how old are you')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# connectSocketIO()"
        }, 
        {
            "execution_count": 27, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Collecting wget\n  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\nBuilding wheels for collected packages: wget\n  Running setup.py bdist_wheel for wget ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\nSuccessfully built wget\nInstalling collected packages: wget\nSuccessfully installed wget-3.2\nCollecting watson-machine-learning-client\n  Downloading https://files.pythonhosted.org/packages/c7/3d/c4e19567fcac15e6aec032b0e1113634793f90081ccd6d6e0a5329fbbc6a/watson_machine_learning_client-1.0.283-py3-none-any.whl (1.0MB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.0MB 1.0MB/s eta 0:00:01\n\u001b[?25hRequirement not upgraded as not directly required: certifi in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-machine-learning-client)\nRequirement not upgraded as not directly required: tqdm in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-machine-learning-client)\nRequirement not upgraded as not directly required: tabulate in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-machine-learning-client)\nRequirement not upgraded as not directly required: urllib3 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-machine-learning-client)\nRequirement not upgraded as not directly required: requests in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-machine-learning-client)\nRequirement not upgraded as not directly required: lomond in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-machine-learning-client)\nRequirement not upgraded as not directly required: ibm-cos-sdk in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-machine-learning-client)\nRequirement not upgraded as not directly required: pandas in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-machine-learning-client)\nRequirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests->watson-machine-learning-client)\nRequirement not upgraded as not directly required: idna<2.7,>=2.5 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests->watson-machine-learning-client)\nRequirement not upgraded as not directly required: six>=1.10.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from lomond->watson-machine-learning-client)\nRequirement not upgraded as not directly required: ibm-cos-sdk-core==2.*,>=2.0.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk->watson-machine-learning-client)\nRequirement not upgraded as not directly required: ibm-cos-sdk-s3transfer==2.*,>=2.0.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk->watson-machine-learning-client)\nRequirement not upgraded as not directly required: python-dateutil>=2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pandas->watson-machine-learning-client)\nRequirement not upgraded as not directly required: pytz>=2011k in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pandas->watson-machine-learning-client)\nRequirement not upgraded as not directly required: numpy>=1.9.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pandas->watson-machine-learning-client)\nRequirement not upgraded as not directly required: docutils>=0.10 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk->watson-machine-learning-client)\nRequirement not upgraded as not directly required: jmespath<1.0.0,>=0.7.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk->watson-machine-learning-client)\nInstalling collected packages: watson-machine-learning-client\n  Found existing installation: watson-machine-learning-client 1.0.241\n    Uninstalling watson-machine-learning-client-1.0.241:\n      Successfully uninstalled watson-machine-learning-client-1.0.241\nSuccessfully installed watson-machine-learning-client-1.0.283\n"
                }
            ], 
            "source": "!pip install wget --upgrade\n!pip install watson-machine-learning-client --upgrade"
        }, 
        {
            "execution_count": 28, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "2018-07-28 06:17:00,276 - watson_machine_learning_client.metanames - WARNING - 'AUTHOR_EMAIL' meta prop is deprecated. It will be ignored.\n2018-07-28 06:17:00,278 - watson_machine_learning_client.wml_client_error - WARNING - Publishing model failed.\nReason: Invalid type for ml_artifact: Sequential\n"
                }
            ], 
            "source": "from watson_machine_learning_client import WatsonMachineLearningAPIClient"
        }, 
        {
            "execution_count": 29, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": 30, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "client = WatsonMachineLearningAPIClient(wml_credentials)"
        }, 
        {
            "execution_count": 31, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "{\n  \"entity\": {\n    \"account\": {\n      \"id\": \"502086283\",\n      \"type\": \"TRIAL\",\n      \"name\": \"\"\n    },\n    \"plan_id\": \"3f6acf43-ede8-413a-ac69-f8af3bb0cbfe\",\n    \"owner\": {\n      \"ibm_id\": \"310001DE7H\",\n      \"user_id\": \"dadd1d5b-b63c-4a8b-b268-8897809227b4\",\n      \"country_code\": \"IND\",\n      \"beta_user\": false,\n      \"email\": \"\"\n    },\n    \"source\": \"Bluemix\",\n    \"plan\": \"lite\",\n    \"published_models\": {\n      \"url\": \"https://us-south.ml.cloud.ibm.com/v3/wml_instances/e7e44faf-ff8d-4183-9f37-434e2dcd6852/published_models\"\n    },\n    \"space_guid\": \"N/A\",\n    \"organization_guid\": \"N/A\",\n    \"usage\": {\n      \"model_count\": {\n        \"limit\": 200,\n        \"current\": 0\n      },\n      \"gpu_count\": {\n        \"limit\": 8,\n        \"current\": 0\n      },\n      \"expiration_date\": \"2018-08-01T00:00:00.000Z\",\n      \"deployment_count\": {\n        \"limit\": 5,\n        \"current\": 0\n      },\n      \"computation_time\": {\n        \"limit\": 180000,\n        \"current\": 300\n      },\n      \"capacity_units\": {\n        \"limit\": 180000000,\n        \"current\": 300002\n      },\n      \"prediction_count\": {\n        \"limit\": 5000,\n        \"current\": 2\n      }\n    },\n    \"region\": \"us-south\",\n    \"status\": \"Active\",\n    \"deployments\": {\n      \"url\": \"https://us-south.ml.cloud.ibm.com/v3/wml_instances/e7e44faf-ff8d-4183-9f37-434e2dcd6852/deployments\"\n    }\n  },\n  \"metadata\": {\n    \"modified_at\": \"2018-06-18T12:44:52.926Z\",\n    \"created_at\": \"2018-06-18T12:44:52.926Z\",\n    \"guid\": \"e7e44faf-ff8d-4183-9f37-434e2dcd6852\",\n    \"url\": \"https://ibm-watson-ml.mybluemix.net/v3/wml_instances/e7e44faf-ff8d-4183-9f37-434e2dcd6852\"\n  }\n}\n"
                }
            ], 
            "source": "instance_details = client.service_instance.get_details()\n\nprint(json.dumps(instance_details, indent=2))"
        }, 
        {
            "execution_count": 32, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "ename": "WMLClientError", 
                    "evalue": "Publishing model failed.\nReason: Invalid type for ml_artifact: Sequential", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/watson_machine_learning_client/repository.py\u001b[0m in \u001b[0;36m_publish_from_object\u001b[0;34m(self, model, meta_props, training_data, training_target, pipeline)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m                 \u001b[0mmodel_artifact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLRepositoryArtifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_props\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelMetaNames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNAME\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_props\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_target\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/watson_machine_learning_client/libs/repository_v3/mlrepositoryartifact/ml_repository_artifact.py\u001b[0m in \u001b[0;36mMLRepositoryArtifact\u001b[0;34m(ml_artifact, training_data, training_dataref, training_target, feature_names, label_column_names, pipeline_artifact, name, meta_props, signature_def_map, tags, assets_collection, legacy_init_op, clear_devices, main_op)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid type for ml_artifact: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mml_artifact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mValueError\u001b[0m: Invalid type for ml_artifact: Sequential", 
                        "\nDuring handling of the above exception, another exception occurred:\n", 
                        "\u001b[0;31mWMLClientError\u001b[0m                            Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-32-f5f6dac80cee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepository\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelMetaNames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNTIME_NAME\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                client.repository.ModelMetaNames.RUNTIME_VERSION: \"3.5\"}\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpublished_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepository\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_props\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_props\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/watson_machine_learning_client/repository.py\u001b[0m in \u001b[0;36mstore_model\u001b[0;34m(self, model, meta_props, training_data, training_target, pipeline)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTR_TYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m             \u001b[0msaved_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_props\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta_props\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_target\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/watson_machine_learning_client/repository.py\u001b[0m in \u001b[0;36m_publish_from_object\u001b[0;34m(self, model, meta_props, training_data, training_target, pipeline)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0msaved_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ml_repository_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_artifact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mWMLClientError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'Publishing model failed.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mWMLClientError\u001b[0m: Publishing model failed.\nReason: Invalid type for ml_artifact: Sequential"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "model_props = {client.repository.ModelMetaNames.AUTHOR_NAME: \"Gurvinder Singh\", \n               client.repository.ModelMetaNames.AUTHOR_EMAIL: \"gurvsin3@in.ibm.com\", \n               client.repository.ModelMetaNames.NAME: \"MyCustomNLCWithKeras\",\n               client.repository.ModelMetaNames.RUNTIME_NAME: \"python\",\n               client.repository.ModelMetaNames.RUNTIME_VERSION: \"3.5\"}\npublished_model = client.repository.store_model(model=model, meta_props=model_props)"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}