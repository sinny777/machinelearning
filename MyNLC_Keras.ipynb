{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "#### Useful Links: \n  * [Watson Nao Robot Notebook](https://github.com/IBM/watson-nao-robot/blob/master/Notebook/Robo_Notebook.ipynb)\n  * [Watson Document Co-Relation](https://github.com/IBM/watson-document-co-relation)\n    ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "1.3.0\n"
                }
            ], 
            "source": "import tensorflow as tf\nprint(tf.__version__)"
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Requirement not upgraded as not directly required: keras in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages\nRequirement not upgraded as not directly required: scipy>=0.14 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from keras)\nRequirement not upgraded as not directly required: numpy>=1.9.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from keras)\nRequirement not upgraded as not directly required: six>=1.9.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from keras)\nRequirement not upgraded as not directly required: pyyaml in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from keras)\nCollecting ibm-cos-sdk\n  Downloading https://files.pythonhosted.org/packages/38/10/b950650fcb95ae6a7c6c7d98de34ddedbc84bb4d5193e6f16cb30917f8c9/ibm-cos-sdk-2.1.2.tar.gz (48kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 6.9MB/s eta 0:00:01\n\u001b[?25hRequirement not upgraded as not directly required: ibm-cos-sdk-core==2.*,>=2.0.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk)\nRequirement not upgraded as not directly required: ibm-cos-sdk-s3transfer==2.*,>=2.0.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk)\nRequirement not upgraded as not directly required: python-dateutil<3.0.0,>=2.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk)\nRequirement not upgraded as not directly required: jmespath<1.0.0,>=0.7.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk)\nRequirement not upgraded as not directly required: docutils>=0.10 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk)\nRequirement not upgraded as not directly required: six>=1.5 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from python-dateutil<3.0.0,>=2.1->ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk)\nBuilding wheels for collected packages: ibm-cos-sdk\n  Running setup.py bdist_wheel for ibm-cos-sdk ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/a2/e5/ea/a39604b956611ea02aec696d1f1157735e873f2f4b43cff2e8\nSuccessfully built ibm-cos-sdk\nInstalling collected packages: ibm-cos-sdk\n  Found existing installation: ibm-cos-sdk 2.0.1\n    Uninstalling ibm-cos-sdk-2.0.1:\n      Successfully uninstalled ibm-cos-sdk-2.0.1\nSuccessfully installed ibm-cos-sdk-2.1.2\nCollecting tflearn\n  Downloading https://files.pythonhosted.org/packages/16/ec/e9ce1b52e71f6dff3bd944f020cef7140779e783ab27512ea7c7275ddee5/tflearn-0.3.2.tar.gz (98kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102kB 6.6MB/s ta 0:00:01\n\u001b[?25hRequirement not upgraded as not directly required: numpy in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from tflearn)\nRequirement not upgraded as not directly required: six in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from tflearn)\nRequirement not upgraded as not directly required: Pillow in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from tflearn)\nRequirement not upgraded as not directly required: olefile in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from Pillow->tflearn)\nBuilding wheels for collected packages: tflearn\n  Running setup.py bdist_wheel for tflearn ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/d0/f6/69/0ef3ee395aac2e5d15d89efd29a9a216f3c27767b43b72c006\nSuccessfully built tflearn\nInstalling collected packages: tflearn\nSuccessfully installed tflearn-0.3.2\nCollecting nltk\n  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.4MB 674kB/s eta 0:00:01\n\u001b[?25hRequirement not upgraded as not directly required: six in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from nltk)\nBuilding wheels for collected packages: nltk\n  Running setup.py bdist_wheel for nltk ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\nSuccessfully built nltk\nInstalling collected packages: nltk\n  Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\nSuccessfully installed nltk-3.3\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/dsxuser/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n[nltk_data] Downloading package punkt to /home/dsxuser/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/dsxuser/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n[nltk_data] Downloading package maxent_ne_chunker to\n[nltk_data]     /home/dsxuser/nltk_data...\n[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n[nltk_data] Downloading package words to /home/dsxuser/nltk_data...\n[nltk_data]   Unzipping corpora/words.zip.\nCollecting socketIO_client_nexus\n  Downloading https://files.pythonhosted.org/packages/a0/fa/2bfd4b5f38530876a26678ed98e3f2233b33479d085cc01adf83af87c3f0/socketIO_client_nexus-0.7.6-py2.py3-none-any.whl\nRequirement not upgraded as not directly required: requests>=2.7.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from socketIO_client_nexus)\nRequirement not upgraded as not directly required: six in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from socketIO_client_nexus)\nCollecting websocket-client (from socketIO_client_nexus)\n  Downloading https://files.pythonhosted.org/packages/8a/a1/72ef9aa26cfe1a75cee09fc1957e4723add9de098c15719416a1ee89386b/websocket_client-0.48.0-py2.py3-none-any.whl (198kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 4.4MB/s eta 0:00:01\n\u001b[?25hRequirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests>=2.7.0->socketIO_client_nexus)\nRequirement not upgraded as not directly required: idna<2.7,>=2.5 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests>=2.7.0->socketIO_client_nexus)\nRequirement not upgraded as not directly required: urllib3<1.23,>=1.21.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests>=2.7.0->socketIO_client_nexus)\nRequirement not upgraded as not directly required: certifi>=2017.4.17 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests>=2.7.0->socketIO_client_nexus)\nInstalling collected packages: websocket-client, socketIO-client-nexus\nSuccessfully installed socketIO-client-nexus-0.7.6 websocket-client-0.48.0\n"
                }
            ], 
            "source": "\n# INSTALL DEPENDENCIES\n# if(tf.__version__ == '1.9.0'):\n#     print(tf.__version__)\n# else:\n#     !pip install --upgrade tensorflow\n#     print(tf.__version__)\n\n!pip install keras\n!pip install -U ibm-cos-sdk\n!pip install tflearn\n!pip install --upgrade nltk\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('maxent_ne_chunker')\nnltk.download('words')\n\n!pip install -U socketIO_client_nexus\n  \nimport pandas as pd\nimport numpy as np\nimport random\n\nimport os.path\nfrom os import path\n\nfrom io import  StringIO\nimport requests\nimport json\nfrom datetime import datetime\nimport time\n\n# things we need for NLP\nimport nltk\nfrom nltk.cluster.util import cosine_distance\nfrom nltk import word_tokenize,sent_tokenize,ne_chunk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.lancaster import LancasterStemmer\nstemmer = LancasterStemmer()\n\nimport sys\nimport types\nfrom botocore.client import Config\nimport ibm_boto3"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "REMOTE = True"
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n# You might want to remove those credentials before you share your notebook.\ndef update_configuration(conf):\n    global config\n    config = conf\n    print(config[\"cos_credentials\"])\n    print(config[\"cos_data\"])\n"
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def multi_part_upload(bucket_name, item_name, file_path):\n    try:\n        print(\"Starting file transfer for {0} to bucket: {1}\\n\".format(item_name, bucket_name))\n        cos = ibm_boto3.resource(service_name='s3',\n            ibm_api_key_id=config[\"cos_credentials\"]['IBM_API_KEY_ID'],\n            ibm_auth_endpoint=config[\"cos_credentials\"]['IBM_AUTH_ENDPOINT'],\n            config=Config(signature_version='oauth'),\n            endpoint_url=config[\"cos_credentials\"]['ENDPOINT'])\n        # set 5 MB chunks\n        part_size = 1024 * 1024 * 5\n\n        # set threadhold to 15 MB\n        file_threshold = 1024 * 1024 * 15\n\n        # set the transfer threshold and chunk size\n        transfer_config = ibm_boto3.s3.transfer.TransferConfig(\n            multipart_threshold=file_threshold,\n            multipart_chunksize=part_size\n        )\n\n        # the upload_fileobj method will automatically execute a multi-part upload \n        # in 5 MB chunks for all files over 15 MB\n        with open(file_path, \"rb\") as file_data:\n            cos.Object(bucket_name, item_name).upload_fileobj(\n                Fileobj=file_data,\n                Config=transfer_config\n            )\n\n        print(\"Transfer for {0} Complete!\\n\".format(item_name))\n    except Exception as e:\n        print(\"Unable to complete multi-part upload: {0}\".format(e))\n"
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def get_object_cos(bucket_name, item_name, path_to_download):\n    try:\n        print(\"Fetching file {0} from bucket: {1}\\n\".format(item_name, bucket_name))\n        cos = ibm_boto3.resource(service_name='s3',\n            ibm_api_key_id=config[\"cos_credentials\"]['IBM_API_KEY_ID'],\n            ibm_auth_endpoint=config[\"cos_credentials\"]['IBM_AUTH_ENDPOINT'],\n            config=Config(signature_version='oauth'),\n            endpoint_url=config[\"cos_credentials\"]['ENDPOINT'])\n        \n        cos.Object(bucket_name, item_name).download_file(path_to_download)\n\n        print(\"Download for {0} Complete!\\n\".format(item_name))\n    except Exception as e:\n        print(\"Unable to download file: {0}\".format(e))\n"
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# LOAD DATA\ndef load_data():\n    global df\n    global cos\n    def __iter__(self): return 0\n\n    # The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n    cos = ibm_boto3.client(service_name='s3',\n        ibm_api_key_id=config[\"cos_credentials\"]['IBM_API_KEY_ID'],\n        ibm_auth_endpoint=config[\"cos_credentials\"]['IBM_AUTH_ENDPOINT'],\n        config=Config(signature_version='oauth'),\n        endpoint_url=config[\"cos_credentials\"]['ENDPOINT'])\n\n    body = cos.get_object(Bucket=config[\"cos_data\"]['BUCKET'],Key=config[\"cos_data\"]['FILE'])['Body']\n    # add missing __iter__ method, so pandas accepts body as file-like object\n    if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\n    df = pd.read_csv(body)\n    df.head()   \n"
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def prepare_documents():\n    global classes\n    global documents\n    global words\n    classes = []\n    documents = []\n    words = []\n    ignore_words = ['?']    \n    \n    # loop through each sentence in our intents patterns\n    for i in range(len(df)):\n        # tokenize each word in the sentence\n        w = nltk.word_tokenize(df[\"utterances\"][i])\n        # add to our words list\n        words.extend(w)\n        # add to documents in our corpus\n        documents.append((w, df[\"intent\"][i]))\n        # add to our classes list\n        if df[\"intent\"][i] not in classes:\n            classes.append(df[\"intent\"][i])\n\n    # stem and lower each word and remove duplicates\n    words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n    words = sorted(list(set(words)))\n\n    # remove duplicates\n    classes = sorted(list(set(classes)))\n\n    print (len(documents), \"documents\")\n    print (len(classes), \"classes\", classes)\n    # print (len(words), \"unique stemmed words\", words)\n"
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# create our training data\ndef prepare_for_training():\n    training = []\n    output = []\n    global train_x\n    global train_y\n    # create an empty array for our output\n    output_empty = [0] * len(classes)\n    # training set, bag of words for each sentence\n    for doc in documents:\n        # initialize our bag of words\n        bag = []\n        # list of tokenized words for the pattern\n        pattern_words = doc[0]\n        # stem each word\n        pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n        # create our bag of words array\n        for w in words:\n            bag.append(1) if w in pattern_words else bag.append(0)\n\n        # output is a '0' for each tag and '1' for current tag\n        output_row = list(output_empty)\n        output_row[classes.index(doc[1])] = 1\n\n        training.append([bag, output_row])\n        \n    # shuffle our features and turn into np.array\n    random.shuffle(training)\n    training = np.array(training)\n    \n    # create train and test lists\n    train_x = list(training[:,0])\n    train_y = list(training[:,1])\n"
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "2.1.4\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "Using TensorFlow backend.\n"
                }
            ], 
            "source": "import keras\nprint(keras.__version__)"
        }, 
        {
            "execution_count": 56, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from keras.models import Sequential\nfrom keras.layers import Dense, Input, concatenate, Activation\n# from tf.keras.layers.pooling import GlobalMaxPooling1D, MaxPooling1D\n# from tf.keras.layers.core import Dropout\nfrom keras import backend as K\n\n# CREATE ML MODEL\ndef create_model():\n    K.clear_session()\n    # tf.global_variables_initializer()\n    tf.reset_default_graph()\n    global model\n    model = Sequential()\n    # model.add(Dense(output_dim=8,init ='uniform',activation='relu', input_dim=len(train_x[0])))\n    model.add(Dense(8, activation='relu', input_shape=(np.asarray(train_x[0]).shape)))\n    # model.add(Dense(8, activation='relu', input_dim=(len(train_x))))\n    # model.add(Activation('relu'))\n    # model.add(Dropout(0.3))\n    model.add(Dense(8, activation='relu'))\n    # model.add(Activation('relu'))\n    # model.add(Dropout(0.3))\n    model.add(Dense(8, activation='relu'))\n    # model.add(Activation('relu'))\n    # model.add(Dropout(0.3))\n    model.add(Dense(np.asarray(train_y[0]).shape[0], activation='softmax'))\n    model.summary()\n    \n    tbCallBack = keras.callbacks.TensorBoard(log_dir='keras_logs', write_graph=True)\n    \n    # model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n    model.fit(np.asarray(train_x), np.asarray(train_y), epochs=200, batch_size=8,  verbose=1, validation_split=0.1, callbacks=[tbCallBack])\n    scores = model.evaluate(np.asarray(train_x), np.asarray(train_y))\n    print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n    model.save('nlc_keras_model.h5')\n    print(\"<<<<<<<< ML MODEL CREATED AND SAVED >>>>>>>>>>>\\n\\n\")"
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def save_model_COS():\n    if(REMOTE):\n        multi_part_upload(config[\"cos_data\"]['BUCKET'], \"model/nlc_keras_model.h5\", \"nlc_keras_model.h5\")\n    "
        }, 
        {
            "execution_count": 57, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def reset_all():\n    update_configuration(conf)\n    load_data()\n    prepare_documents()\n    prepare_for_training()\n    create_model()\n    save_model_COS()\n"
        }, 
        {
            "source": "# Code to Classify text using the ML Model created", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 17, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def fetch_ml_model_cos():\n    if(path.exists('nlc_keras_model.h5') == False):\n        get_object_cos(config[\"cos_data\"]['BUCKET'], \"model/nlc_keras_model.h5\", \"nlc_keras_model.h5\")    \n"
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from keras.models import load_model\ndef load_ml_model():\n    global model\n    try:\n        model\n    except NameError:\n        print(\"<<< ML Model Needs to be loaded >>>>>\")\n        # load our saved model\n        fetch_ml_model_cos()\n        model = load_model('nlc_keras_model.h5')        \n    else:\n        print(\"<<< ML Model Already Exists >>>>>\")        \n"
        }, 
        {
            "execution_count": 19, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def clean_up_sentence(sentence):\n    # tokenize the pattern\n    sentence_words = nltk.word_tokenize(sentence)\n    # stem each word\n    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n    return sentence_words\n\n# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\ndef bow(sentence, words, show_details=False):\n    # tokenize the pattern\n    sentence_words = clean_up_sentence(sentence)\n    # bag of words\n    bag = [0]*len(words)  \n    for s in sentence_words:\n        for i,w in enumerate(words):\n            if w == s: \n                bag[i] = 1\n                if show_details:\n                    print (\"found in bag: %s\" % w)\n\n    return(np.array(bag))"
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# create a data structure to hold user context\ncontext = {}\n\nERROR_THRESHOLD = 0.25\ndef classify(sentence):\n    # generate probabilities from the model\n    load_ml_model()\n    to_predict = bow(sentence, words)\n    if (to_predict.ndim == 1):\n        to_predict = np.array([to_predict])\n    \n    results = model.predict([to_predict])[0]\n    # filter out predictions below a threshold\n    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n    # sort by strength of probability\n    results.sort(key=lambda x: x[1], reverse=True)\n    return_list = []\n    for r in results:\n        return_list.append((classes[r[0]], r[1]))\n    # return tuple of intent and probability\n    return return_list"
        }, 
        {
            "execution_count": 21, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def reset_for_classification():\n    update_configuration(conf)\n    load_data()\n    prepare_documents()\n    prepare_for_training()"
        }, 
        {
            "execution_count": 22, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from socketIO_client_nexus import SocketIO, BaseNamespace, LoggingNamespace\n\ndef on_connect():\n    print('on_connect')\n\ndef on_disconnect():\n    print('on_disconnect')\n\ndef on_reconnect():\n    print('on_reconnect')\n\ndef on_response(*message):\n    msg = json.loads(json.dumps(message))\n    print(type(msg))\n    print('\\n\\non_response: >> ', msg[0])\n    command = msg[0][\"command\"]\n    params = msg[0][\"params\"]\n    if command == \"reset_all\":\n        reset_all()\n    elif command == \"classify\":\n        results = classify(params[\"text\"])\n        print(results)\n    else:\n        print(\"Command not recognized....\")\n\ndef connectSocketIO():\n#     SocketIO('https://localhost', verify=False)\n    with SocketIO('https://my-watson-assistant-api.mybluemix.net', verify=False) as socketIO:\n        # with SocketIO('localhost', verify=False) as socketIO:\n        socketIO.on('connect', on_connect)\n        socketIO.on('disconnect', on_disconnect)\n        socketIO.on('reconnect', on_reconnect)\n        socketIO.on('/ml', on_response)\n        socketIO.wait()\n"
        }, 
        {
            "execution_count": 58, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "{'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com', 'IBM_AUTH_ENDPOINT': 'https://iam.ng.bluemix.net/oidc/token', 'IBM_API_KEY_ID': 'jDLQvkwwo3h77B5MWgqOTUq25D94Xr6CGrb_6dYmVcj-'}\n{'FILE': 'raw_car_dashboard_ml.csv', 'BUCKET': 'myml-donotdelete-pr-zhsoop3fasxh7h'}\n2532 documents\n26 classes ['about_VA', 'capabilites', 'capabilities', 'compound_questions', 'decision_replies', 'goodbyes', 'greetings', 'improving_system', 'information_request', 'interface_interactions', 'interface_issues', 'locate_amenity', 'navigation', 'negative_reaction', 'not_specified', 'out_of_scope', 'phone', 'positive_reaction', 'selections', 'system_reliance', 'traffic_update', 'turn_down', 'turn_off', 'turn_on', 'turn_up', 'weather']\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 8)                 10656     \n_________________________________________________________________\ndense_2 (Dense)              (None, 8)                 72        \n_________________________________________________________________\ndense_3 (Dense)              (None, 8)                 72        \n_________________________________________________________________\ndense_4 (Dense)              (None, 26)                234       \n=================================================================\nTotal params: 11,034\nTrainable params: 11,034\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 2278 samples, validate on 254 samples\nEpoch 1/200\n2278/2278 [==============================] - 1s 487us/step - loss: 2.7778 - acc: 0.4311 - val_loss: 2.2150 - val_acc: 0.5157\nEpoch 2/200\n2278/2278 [==============================] - 1s 487us/step - loss: 1.8389 - acc: 0.5522 - val_loss: 1.7625 - val_acc: 0.5354\nEpoch 3/200\n2278/2278 [==============================] - 1s 509us/step - loss: 1.5335 - acc: 0.5584 - val_loss: 1.5802 - val_acc: 0.5394\nEpoch 4/200\n2278/2278 [==============================] - 1s 483us/step - loss: 1.3904 - acc: 0.5619 - val_loss: 1.4930 - val_acc: 0.5394\nEpoch 5/200\n2278/2278 [==============================] - 1s 485us/step - loss: 1.2892 - acc: 0.5935 - val_loss: 1.4385 - val_acc: 0.5630\nEpoch 6/200\n2278/2278 [==============================] - 1s 488us/step - loss: 1.2058 - acc: 0.6356 - val_loss: 1.3851 - val_acc: 0.5866\nEpoch 7/200\n2278/2278 [==============================] - 1s 488us/step - loss: 1.1365 - acc: 0.6765 - val_loss: 1.3574 - val_acc: 0.6063\nEpoch 8/200\n2278/2278 [==============================] - 1s 510us/step - loss: 1.0830 - acc: 0.6861 - val_loss: 1.3254 - val_acc: 0.6339\nEpoch 9/200\n2278/2278 [==============================] - 1s 487us/step - loss: 1.0284 - acc: 0.7006 - val_loss: 1.3051 - val_acc: 0.6378\nEpoch 10/200\n2278/2278 [==============================] - 1s 486us/step - loss: 0.9800 - acc: 0.7094 - val_loss: 1.2785 - val_acc: 0.6457\nEpoch 11/200\n2278/2278 [==============================] - 1s 489us/step - loss: 0.9384 - acc: 0.7230 - val_loss: 1.2604 - val_acc: 0.6772\nEpoch 12/200\n2278/2278 [==============================] - 1s 491us/step - loss: 0.9042 - acc: 0.7432 - val_loss: 1.2355 - val_acc: 0.6850\nEpoch 13/200\n2278/2278 [==============================] - 1s 486us/step - loss: 0.8697 - acc: 0.7520 - val_loss: 1.2411 - val_acc: 0.6850\nEpoch 14/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.8425 - acc: 0.7638 - val_loss: 1.2248 - val_acc: 0.6850\nEpoch 15/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.8155 - acc: 0.7735 - val_loss: 1.2226 - val_acc: 0.7047\nEpoch 16/200\n2278/2278 [==============================] - 1s 514us/step - loss: 0.7898 - acc: 0.7801 - val_loss: 1.2338 - val_acc: 0.7047\nEpoch 17/200\n2278/2278 [==============================] - 1s 489us/step - loss: 0.7684 - acc: 0.7849 - val_loss: 1.2115 - val_acc: 0.7205\nEpoch 18/200\n2278/2278 [==============================] - 1s 488us/step - loss: 0.7452 - acc: 0.7932 - val_loss: 1.2255 - val_acc: 0.7165\nEpoch 19/200\n2278/2278 [==============================] - 1s 550us/step - loss: 0.7296 - acc: 0.7972 - val_loss: 1.2326 - val_acc: 0.7283\nEpoch 20/200\n2278/2278 [==============================] - 1s 490us/step - loss: 0.7088 - acc: 0.8047 - val_loss: 1.2331 - val_acc: 0.7205\nEpoch 21/200\n2278/2278 [==============================] - 1s 487us/step - loss: 0.6898 - acc: 0.8090 - val_loss: 1.2307 - val_acc: 0.7283\nEpoch 22/200\n2278/2278 [==============================] - 1s 483us/step - loss: 0.6691 - acc: 0.8156 - val_loss: 1.2232 - val_acc: 0.7441\nEpoch 23/200\n2278/2278 [==============================] - 1s 481us/step - loss: 0.6518 - acc: 0.8306 - val_loss: 1.2446 - val_acc: 0.7402\nEpoch 24/200\n2278/2278 [==============================] - 1s 484us/step - loss: 0.6406 - acc: 0.8358 - val_loss: 1.2062 - val_acc: 0.7441\nEpoch 25/200\n2278/2278 [==============================] - 1s 489us/step - loss: 0.6211 - acc: 0.8380 - val_loss: 1.2156 - val_acc: 0.7362\nEpoch 26/200\n2278/2278 [==============================] - 1s 480us/step - loss: 0.6112 - acc: 0.8446 - val_loss: 1.2278 - val_acc: 0.7323\nEpoch 27/200\n2278/2278 [==============================] - 1s 482us/step - loss: 0.5953 - acc: 0.8468 - val_loss: 1.2488 - val_acc: 0.7402\nEpoch 28/200\n2278/2278 [==============================] - 1s 520us/step - loss: 0.5840 - acc: 0.8494 - val_loss: 1.2372 - val_acc: 0.7323\nEpoch 29/200\n2278/2278 [==============================] - 1s 484us/step - loss: 0.5707 - acc: 0.8507 - val_loss: 1.2190 - val_acc: 0.7323\nEpoch 30/200\n2278/2278 [==============================] - 1s 480us/step - loss: 0.5560 - acc: 0.8661 - val_loss: 1.2516 - val_acc: 0.7441\nEpoch 31/200\n2278/2278 [==============================] - 1s 489us/step - loss: 0.5497 - acc: 0.8687 - val_loss: 1.2485 - val_acc: 0.7441\nEpoch 32/200\n2278/2278 [==============================] - 1s 509us/step - loss: 0.5372 - acc: 0.8657 - val_loss: 1.2383 - val_acc: 0.7402\nEpoch 33/200\n2278/2278 [==============================] - 1s 488us/step - loss: 0.5259 - acc: 0.8753 - val_loss: 1.2543 - val_acc: 0.7520\nEpoch 34/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.5139 - acc: 0.8683 - val_loss: 1.2662 - val_acc: 0.7480\nEpoch 35/200\n2278/2278 [==============================] - 1s 489us/step - loss: 0.5067 - acc: 0.8806 - val_loss: 1.2775 - val_acc: 0.7559\nEpoch 36/200\n2278/2278 [==============================] - 1s 509us/step - loss: 0.4944 - acc: 0.8806 - val_loss: 1.2573 - val_acc: 0.7441\nEpoch 37/200\n2278/2278 [==============================] - 1s 491us/step - loss: 0.4853 - acc: 0.8824 - val_loss: 1.2878 - val_acc: 0.7598\nEpoch 38/200\n2278/2278 [==============================] - 1s 512us/step - loss: 0.4791 - acc: 0.8854 - val_loss: 1.2847 - val_acc: 0.7598\nEpoch 39/200\n2278/2278 [==============================] - 1s 490us/step - loss: 0.4663 - acc: 0.8885 - val_loss: 1.2665 - val_acc: 0.7677\nEpoch 40/200\n2278/2278 [==============================] - 1s 481us/step - loss: 0.4566 - acc: 0.8881 - val_loss: 1.3110 - val_acc: 0.7598\nEpoch 41/200\n2278/2278 [==============================] - 1s 489us/step - loss: 0.4463 - acc: 0.8938 - val_loss: 1.2865 - val_acc: 0.7638\nEpoch 42/200\n2278/2278 [==============================] - 1s 486us/step - loss: 0.4402 - acc: 0.8982 - val_loss: 1.3086 - val_acc: 0.7638\nEpoch 43/200\n2278/2278 [==============================] - 1s 508us/step - loss: 0.4321 - acc: 0.8973 - val_loss: 1.3161 - val_acc: 0.7677\nEpoch 44/200\n2278/2278 [==============================] - 1s 484us/step - loss: 0.4245 - acc: 0.8968 - val_loss: 1.2982 - val_acc: 0.7598\nEpoch 45/200\n2278/2278 [==============================] - 1s 492us/step - loss: 0.4165 - acc: 0.8982 - val_loss: 1.3157 - val_acc: 0.7717\nEpoch 46/200\n2278/2278 [==============================] - 1s 511us/step - loss: 0.4095 - acc: 0.9008 - val_loss: 1.3118 - val_acc: 0.7677\nEpoch 47/200\n2278/2278 [==============================] - 1s 482us/step - loss: 0.3996 - acc: 0.9034 - val_loss: 1.3037 - val_acc: 0.7638\nEpoch 48/200\n2278/2278 [==============================] - 1s 487us/step - loss: 0.3933 - acc: 0.9052 - val_loss: 1.3198 - val_acc: 0.7717\nEpoch 49/200\n2278/2278 [==============================] - 1s 481us/step - loss: 0.3864 - acc: 0.9052 - val_loss: 1.3323 - val_acc: 0.7756\nEpoch 50/200\n2278/2278 [==============================] - 1s 484us/step - loss: 0.3784 - acc: 0.9047 - val_loss: 1.3522 - val_acc: 0.7717\nEpoch 51/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.3738 - acc: 0.9083 - val_loss: 1.3126 - val_acc: 0.7638\nEpoch 52/200\n2278/2278 [==============================] - 1s 487us/step - loss: 0.3696 - acc: 0.9109 - val_loss: 1.3409 - val_acc: 0.7677\nEpoch 53/200\n2278/2278 [==============================] - 1s 488us/step - loss: 0.3588 - acc: 0.9118 - val_loss: 1.3567 - val_acc: 0.7677\nEpoch 54/200\n2278/2278 [==============================] - 1s 507us/step - loss: 0.3488 - acc: 0.9122 - val_loss: 1.3807 - val_acc: 0.7638\nEpoch 55/200\n2278/2278 [==============================] - 1s 491us/step - loss: 0.3435 - acc: 0.9157 - val_loss: 1.3220 - val_acc: 0.7677\nEpoch 56/200\n2278/2278 [==============================] - 1s 484us/step - loss: 0.3374 - acc: 0.9153 - val_loss: 1.3143 - val_acc: 0.7795\nEpoch 57/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.3327 - acc: 0.9183 - val_loss: 1.3320 - val_acc: 0.7835\nEpoch 58/200\n2278/2278 [==============================] - 1s 487us/step - loss: 0.3253 - acc: 0.9188 - val_loss: 1.3299 - val_acc: 0.7795\nEpoch 59/200\n2278/2278 [==============================] - 1s 482us/step - loss: 0.3168 - acc: 0.9210 - val_loss: 1.3358 - val_acc: 0.7756\nEpoch 60/200\n2278/2278 [==============================] - 1s 513us/step - loss: 0.3116 - acc: 0.9214 - val_loss: 1.3521 - val_acc: 0.7677\nEpoch 61/200\n2278/2278 [==============================] - 1s 494us/step - loss: 0.3088 - acc: 0.9210 - val_loss: 1.3584 - val_acc: 0.7677\nEpoch 62/200\n2278/2278 [==============================] - 1s 517us/step - loss: 0.3015 - acc: 0.9223 - val_loss: 1.3745 - val_acc: 0.7717\nEpoch 63/200\n2278/2278 [==============================] - 1s 483us/step - loss: 0.2993 - acc: 0.9232 - val_loss: 1.4004 - val_acc: 0.7638\nEpoch 64/200\n2278/2278 [==============================] - 1s 489us/step - loss: 0.2932 - acc: 0.9254 - val_loss: 1.3796 - val_acc: 0.7717\nEpoch 65/200\n2278/2278 [==============================] - 1s 478us/step - loss: 0.2888 - acc: 0.9298 - val_loss: 1.3919 - val_acc: 0.7717\nEpoch 66/200\n2278/2278 [==============================] - 1s 481us/step - loss: 0.2833 - acc: 0.9293 - val_loss: 1.3761 - val_acc: 0.7756\nEpoch 67/200\n2278/2278 [==============================] - 1s 479us/step - loss: 0.2779 - acc: 0.9289 - val_loss: 1.3975 - val_acc: 0.7717\nEpoch 68/200\n2278/2278 [==============================] - 1s 484us/step - loss: 0.2738 - acc: 0.9298 - val_loss: 1.4156 - val_acc: 0.7598\nEpoch 69/200\n2278/2278 [==============================] - 1s 487us/step - loss: 0.2703 - acc: 0.9289 - val_loss: 1.4106 - val_acc: 0.7559\nEpoch 70/200\n2278/2278 [==============================] - 1s 511us/step - loss: 0.2699 - acc: 0.9276 - val_loss: 1.4169 - val_acc: 0.7598\nEpoch 71/200\n2278/2278 [==============================] - 1s 479us/step - loss: 0.2644 - acc: 0.9306 - val_loss: 1.4125 - val_acc: 0.7559\nEpoch 72/200\n2278/2278 [==============================] - 1s 484us/step - loss: 0.2597 - acc: 0.9324 - val_loss: 1.4351 - val_acc: 0.7598\nEpoch 73/200\n2278/2278 [==============================] - 1s 491us/step - loss: 0.2573 - acc: 0.9346 - val_loss: 1.4401 - val_acc: 0.7677\nEpoch 74/200\n2278/2278 [==============================] - 1s 487us/step - loss: 0.2544 - acc: 0.9342 - val_loss: 1.4077 - val_acc: 0.7638\nEpoch 75/200\n2278/2278 [==============================] - 1s 511us/step - loss: 0.2493 - acc: 0.9333 - val_loss: 1.4542 - val_acc: 0.7638\nEpoch 76/200\n2278/2278 [==============================] - 1s 482us/step - loss: 0.2474 - acc: 0.9368 - val_loss: 1.4423 - val_acc: 0.7638\nEpoch 77/200\n2278/2278 [==============================] - 1s 451us/step - loss: 0.2451 - acc: 0.9342 - val_loss: 1.4554 - val_acc: 0.7677\nEpoch 78/200\n2278/2278 [==============================] - 1s 512us/step - loss: 0.2415 - acc: 0.9372 - val_loss: 1.4552 - val_acc: 0.7717\nEpoch 79/200\n2278/2278 [==============================] - 1s 481us/step - loss: 0.2388 - acc: 0.9355 - val_loss: 1.4394 - val_acc: 0.7638\nEpoch 80/200\n2278/2278 [==============================] - 1s 484us/step - loss: 0.2378 - acc: 0.9359 - val_loss: 1.4593 - val_acc: 0.7598\nEpoch 81/200\n2278/2278 [==============================] - 1s 488us/step - loss: 0.2341 - acc: 0.9368 - val_loss: 1.4609 - val_acc: 0.7520\nEpoch 82/200\n2278/2278 [==============================] - 1s 482us/step - loss: 0.2322 - acc: 0.9372 - val_loss: 1.4519 - val_acc: 0.7559\nEpoch 83/200\n2278/2278 [==============================] - 1s 487us/step - loss: 0.2293 - acc: 0.9377 - val_loss: 1.4667 - val_acc: 0.7480\nEpoch 84/200\n2278/2278 [==============================] - 1s 486us/step - loss: 0.2267 - acc: 0.9399 - val_loss: 1.4832 - val_acc: 0.7559\nEpoch 85/200\n2278/2278 [==============================] - 1s 512us/step - loss: 0.2240 - acc: 0.9394 - val_loss: 1.5048 - val_acc: 0.7598\nEpoch 86/200\n2278/2278 [==============================] - 1s 482us/step - loss: 0.2205 - acc: 0.9399 - val_loss: 1.5299 - val_acc: 0.7480\nEpoch 87/200\n2278/2278 [==============================] - 1s 489us/step - loss: 0.2161 - acc: 0.9434 - val_loss: 1.5129 - val_acc: 0.7323\nEpoch 88/200\n2278/2278 [==============================] - 1s 487us/step - loss: 0.2164 - acc: 0.9447 - val_loss: 1.5057 - val_acc: 0.7520\nEpoch 89/200\n2278/2278 [==============================] - 1s 515us/step - loss: 0.2134 - acc: 0.9434 - val_loss: 1.5209 - val_acc: 0.7520\nEpoch 90/200\n2278/2278 [==============================] - 1s 483us/step - loss: 0.2128 - acc: 0.9442 - val_loss: 1.5276 - val_acc: 0.7323\nEpoch 91/200\n2278/2278 [==============================] - 1s 478us/step - loss: 0.2089 - acc: 0.9434 - val_loss: 1.5525 - val_acc: 0.7165\nEpoch 92/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.2086 - acc: 0.9447 - val_loss: 1.5615 - val_acc: 0.7402\nEpoch 93/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.2069 - acc: 0.9438 - val_loss: 1.5642 - val_acc: 0.7480\nEpoch 94/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.2033 - acc: 0.9447 - val_loss: 1.5826 - val_acc: 0.7559\nEpoch 95/200\n2278/2278 [==============================] - 1s 488us/step - loss: 0.2019 - acc: 0.9464 - val_loss: 1.5897 - val_acc: 0.7520\nEpoch 96/200\n2278/2278 [==============================] - 1s 515us/step - loss: 0.1985 - acc: 0.9442 - val_loss: 1.5879 - val_acc: 0.7441\nEpoch 97/200\n2278/2278 [==============================] - 1s 480us/step - loss: 0.1959 - acc: 0.9478 - val_loss: 1.5871 - val_acc: 0.7480\nEpoch 98/200\n2278/2278 [==============================] - 1s 486us/step - loss: 0.1968 - acc: 0.9429 - val_loss: 1.6025 - val_acc: 0.7520\nEpoch 99/200\n2278/2278 [==============================] - 1s 488us/step - loss: 0.1935 - acc: 0.9460 - val_loss: 1.6163 - val_acc: 0.7323\nEpoch 100/200\n2278/2278 [==============================] - 1s 520us/step - loss: 0.1924 - acc: 0.9451 - val_loss: 1.6262 - val_acc: 0.7402\nEpoch 101/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.1920 - acc: 0.9469 - val_loss: 1.6591 - val_acc: 0.7323\nEpoch 102/200\n2278/2278 [==============================] - 1s 482us/step - loss: 0.1896 - acc: 0.9464 - val_loss: 1.6558 - val_acc: 0.7362\nEpoch 103/200\n2278/2278 [==============================] - 1s 483us/step - loss: 0.1859 - acc: 0.9491 - val_loss: 1.6692 - val_acc: 0.7402\nEpoch 104/200\n2278/2278 [==============================] - 1s 509us/step - loss: 0.1828 - acc: 0.9486 - val_loss: 1.6917 - val_acc: 0.7323\nEpoch 105/200\n2278/2278 [==============================] - 1s 487us/step - loss: 0.1838 - acc: 0.9486 - val_loss: 1.6923 - val_acc: 0.7362\nEpoch 106/200\n2278/2278 [==============================] - 1s 481us/step - loss: 0.1830 - acc: 0.9491 - val_loss: 1.6677 - val_acc: 0.7441\nEpoch 107/200\n2278/2278 [==============================] - 1s 487us/step - loss: 0.1817 - acc: 0.9482 - val_loss: 1.7047 - val_acc: 0.7323\nEpoch 108/200\n2278/2278 [==============================] - 1s 513us/step - loss: 0.1779 - acc: 0.9478 - val_loss: 1.7494 - val_acc: 0.7283\nEpoch 109/200\n2278/2278 [==============================] - 1s 494us/step - loss: 0.1763 - acc: 0.9500 - val_loss: 1.7476 - val_acc: 0.7402\nEpoch 110/200\n2278/2278 [==============================] - 1s 489us/step - loss: 0.1739 - acc: 0.9513 - val_loss: 1.7127 - val_acc: 0.7402\nEpoch 111/200\n2278/2278 [==============================] - 1s 481us/step - loss: 0.1745 - acc: 0.9513 - val_loss: 1.7571 - val_acc: 0.7402\nEpoch 112/200\n2278/2278 [==============================] - 1s 512us/step - loss: 0.1736 - acc: 0.9530 - val_loss: 1.7659 - val_acc: 0.7402\nEpoch 113/200\n2278/2278 [==============================] - 1s 480us/step - loss: 0.1687 - acc: 0.9543 - val_loss: 1.7918 - val_acc: 0.7402\nEpoch 114/200\n2278/2278 [==============================] - 1s 488us/step - loss: 0.1676 - acc: 0.9557 - val_loss: 1.8040 - val_acc: 0.7441\nEpoch 115/200\n2278/2278 [==============================] - 1s 478us/step - loss: 0.1660 - acc: 0.9557 - val_loss: 1.8147 - val_acc: 0.7402\nEpoch 116/200\n2278/2278 [==============================] - 1s 480us/step - loss: 0.1702 - acc: 0.9539 - val_loss: 1.8081 - val_acc: 0.7402\nEpoch 117/200\n2278/2278 [==============================] - 1s 483us/step - loss: 0.1673 - acc: 0.9557 - val_loss: 1.8222 - val_acc: 0.7441\nEpoch 118/200\n2278/2278 [==============================] - 1s 491us/step - loss: 0.1647 - acc: 0.9574 - val_loss: 1.8395 - val_acc: 0.7323\nEpoch 119/200\n2278/2278 [==============================] - 1s 486us/step - loss: 0.1631 - acc: 0.9557 - val_loss: 1.8438 - val_acc: 0.7441\nEpoch 120/200\n2278/2278 [==============================] - 1s 482us/step - loss: 0.1621 - acc: 0.9565 - val_loss: 1.8765 - val_acc: 0.7283\nEpoch 121/200\n2278/2278 [==============================] - 1s 482us/step - loss: 0.1602 - acc: 0.9592 - val_loss: 1.8532 - val_acc: 0.7441\nEpoch 122/200\n2278/2278 [==============================] - 1s 512us/step - loss: 0.1585 - acc: 0.9565 - val_loss: 1.9010 - val_acc: 0.7402\nEpoch 123/200\n2278/2278 [==============================] - 1s 492us/step - loss: 0.1598 - acc: 0.9596 - val_loss: 1.9189 - val_acc: 0.7323\nEpoch 124/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.1593 - acc: 0.9614 - val_loss: 1.9185 - val_acc: 0.7323\nEpoch 125/200\n2278/2278 [==============================] - 1s 487us/step - loss: 0.1559 - acc: 0.9574 - val_loss: 1.9337 - val_acc: 0.7283\nEpoch 126/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.1561 - acc: 0.9609 - val_loss: 1.9247 - val_acc: 0.7283\nEpoch 127/200\n2278/2278 [==============================] - 1s 490us/step - loss: 0.1531 - acc: 0.9618 - val_loss: 1.9160 - val_acc: 0.7362\nEpoch 128/200\n2278/2278 [==============================] - 1s 483us/step - loss: 0.1530 - acc: 0.9618 - val_loss: 1.9123 - val_acc: 0.7362\nEpoch 129/200\n2278/2278 [==============================] - 1s 487us/step - loss: 0.1496 - acc: 0.9622 - val_loss: 1.9136 - val_acc: 0.7323\nEpoch 130/200\n2278/2278 [==============================] - 1s 515us/step - loss: 0.1512 - acc: 0.9622 - val_loss: 1.9915 - val_acc: 0.7323\nEpoch 131/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.1499 - acc: 0.9627 - val_loss: 2.0088 - val_acc: 0.7087\nEpoch 132/200\n2278/2278 [==============================] - 1s 484us/step - loss: 0.1476 - acc: 0.9622 - val_loss: 1.9829 - val_acc: 0.7283\nEpoch 133/200\n2278/2278 [==============================] - 1s 480us/step - loss: 0.1465 - acc: 0.9658 - val_loss: 2.0036 - val_acc: 0.7244\nEpoch 134/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.1458 - acc: 0.9658 - val_loss: 1.9795 - val_acc: 0.7244\nEpoch 135/200\n2278/2278 [==============================] - 1s 480us/step - loss: 0.1460 - acc: 0.9662 - val_loss: 2.0071 - val_acc: 0.7244\nEpoch 136/200\n2278/2278 [==============================] - 1s 486us/step - loss: 0.1425 - acc: 0.9649 - val_loss: 2.0445 - val_acc: 0.7323\nEpoch 137/200\n2278/2278 [==============================] - 1s 510us/step - loss: 0.1422 - acc: 0.9653 - val_loss: 2.0334 - val_acc: 0.7205\nEpoch 138/200\n2278/2278 [==============================] - 1s 487us/step - loss: 0.1418 - acc: 0.9640 - val_loss: 2.0435 - val_acc: 0.7205\nEpoch 139/200\n2278/2278 [==============================] - 1s 486us/step - loss: 0.1415 - acc: 0.9666 - val_loss: 2.0581 - val_acc: 0.7323\nEpoch 140/200\n2278/2278 [==============================] - 1s 484us/step - loss: 0.1409 - acc: 0.9666 - val_loss: 2.0885 - val_acc: 0.7244\nEpoch 141/200\n2278/2278 [==============================] - 1s 515us/step - loss: 0.1398 - acc: 0.9671 - val_loss: 2.0527 - val_acc: 0.7205\nEpoch 142/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.1405 - acc: 0.9671 - val_loss: 2.1331 - val_acc: 0.7205\nEpoch 143/200\n2278/2278 [==============================] - 1s 488us/step - loss: 0.1388 - acc: 0.9666 - val_loss: 2.1363 - val_acc: 0.7087\nEpoch 144/200\n2278/2278 [==============================] - 1s 513us/step - loss: 0.1357 - acc: 0.9675 - val_loss: 2.1734 - val_acc: 0.7165\nEpoch 145/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.1362 - acc: 0.9653 - val_loss: 2.1656 - val_acc: 0.7165\nEpoch 146/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.1363 - acc: 0.9688 - val_loss: 2.1493 - val_acc: 0.7165\nEpoch 147/200\n2278/2278 [==============================] - 1s 484us/step - loss: 0.1343 - acc: 0.9680 - val_loss: 2.1874 - val_acc: 0.7244\nEpoch 148/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.1313 - acc: 0.9653 - val_loss: 2.2041 - val_acc: 0.7205\nEpoch 149/200\n2278/2278 [==============================] - 1s 482us/step - loss: 0.1298 - acc: 0.9706 - val_loss: 2.1891 - val_acc: 0.7165\nEpoch 150/200\n2278/2278 [==============================] - 1s 484us/step - loss: 0.1303 - acc: 0.9684 - val_loss: 2.1967 - val_acc: 0.7205\nEpoch 151/200\n2278/2278 [==============================] - 1s 511us/step - loss: 0.1276 - acc: 0.9693 - val_loss: 2.2041 - val_acc: 0.7205\nEpoch 152/200\n2278/2278 [==============================] - 1s 492us/step - loss: 0.1289 - acc: 0.9680 - val_loss: 2.2374 - val_acc: 0.7126\nEpoch 153/200\n2278/2278 [==============================] - 1s 484us/step - loss: 0.1250 - acc: 0.9688 - val_loss: 2.2481 - val_acc: 0.7165\nEpoch 154/200\n2278/2278 [==============================] - 1s 480us/step - loss: 0.1286 - acc: 0.9688 - val_loss: 2.3496 - val_acc: 0.7087\nEpoch 155/200\n2278/2278 [==============================] - 1s 511us/step - loss: 0.1254 - acc: 0.9715 - val_loss: 2.3443 - val_acc: 0.7126\nEpoch 156/200\n2278/2278 [==============================] - 1s 484us/step - loss: 0.1245 - acc: 0.9706 - val_loss: 2.3090 - val_acc: 0.7126\nEpoch 157/200\n2278/2278 [==============================] - 1s 487us/step - loss: 0.1244 - acc: 0.9701 - val_loss: 2.3173 - val_acc: 0.7126\nEpoch 158/200\n2278/2278 [==============================] - 1s 490us/step - loss: 0.1218 - acc: 0.9701 - val_loss: 2.3180 - val_acc: 0.7126\nEpoch 159/200\n2278/2278 [==============================] - 1s 513us/step - loss: 0.1223 - acc: 0.9715 - val_loss: 2.3032 - val_acc: 0.7244\nEpoch 160/200\n2278/2278 [==============================] - 1s 487us/step - loss: 0.1207 - acc: 0.9697 - val_loss: 2.3078 - val_acc: 0.7165\nEpoch 161/200\n2278/2278 [==============================] - 1s 483us/step - loss: 0.1203 - acc: 0.9706 - val_loss: 2.3763 - val_acc: 0.7047\nEpoch 162/200\n2278/2278 [==============================] - 1s 490us/step - loss: 0.1182 - acc: 0.9719 - val_loss: 2.3659 - val_acc: 0.7087\nEpoch 163/200\n2278/2278 [==============================] - 1s 488us/step - loss: 0.1183 - acc: 0.9710 - val_loss: 2.3921 - val_acc: 0.7047\nEpoch 164/200\n2278/2278 [==============================] - 1s 490us/step - loss: 0.1142 - acc: 0.9728 - val_loss: 2.4359 - val_acc: 0.7008\nEpoch 165/200\n2278/2278 [==============================] - 1s 487us/step - loss: 0.1160 - acc: 0.9715 - val_loss: 2.4069 - val_acc: 0.6969\nEpoch 166/200\n2278/2278 [==============================] - 1s 516us/step - loss: 0.1122 - acc: 0.9710 - val_loss: 2.4349 - val_acc: 0.7047\nEpoch 167/200\n2278/2278 [==============================] - 1s 478us/step - loss: 0.1113 - acc: 0.9732 - val_loss: 2.4899 - val_acc: 0.6929\nEpoch 168/200\n2278/2278 [==============================] - 1s 487us/step - loss: 0.1104 - acc: 0.9723 - val_loss: 2.4542 - val_acc: 0.7008\nEpoch 169/200\n2278/2278 [==============================] - 1s 486us/step - loss: 0.1107 - acc: 0.9715 - val_loss: 2.4707 - val_acc: 0.7087\nEpoch 170/200\n2278/2278 [==============================] - 1s 482us/step - loss: 0.1114 - acc: 0.9728 - val_loss: 2.4356 - val_acc: 0.7087\nEpoch 171/200\n2278/2278 [==============================] - 1s 483us/step - loss: 0.1092 - acc: 0.9737 - val_loss: 2.4914 - val_acc: 0.6890\nEpoch 172/200\n2278/2278 [==============================] - 1s 479us/step - loss: 0.1100 - acc: 0.9723 - val_loss: 2.4547 - val_acc: 0.7047\nEpoch 173/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.1081 - acc: 0.9767 - val_loss: 2.4314 - val_acc: 0.7008\nEpoch 174/200\n2278/2278 [==============================] - 1s 510us/step - loss: 0.1059 - acc: 0.9741 - val_loss: 2.4659 - val_acc: 0.7087\nEpoch 175/200\n2278/2278 [==============================] - 1s 486us/step - loss: 0.1064 - acc: 0.9741 - val_loss: 2.5245 - val_acc: 0.7008\nEpoch 176/200\n2278/2278 [==============================] - 1s 489us/step - loss: 0.1035 - acc: 0.9745 - val_loss: 2.5198 - val_acc: 0.7047\nEpoch 177/200\n2278/2278 [==============================] - 1s 481us/step - loss: 0.1046 - acc: 0.9763 - val_loss: 2.5562 - val_acc: 0.6969\nEpoch 178/200\n2278/2278 [==============================] - 1s 487us/step - loss: 0.1046 - acc: 0.9750 - val_loss: 2.5155 - val_acc: 0.6929\nEpoch 179/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.1019 - acc: 0.9750 - val_loss: 2.5886 - val_acc: 0.6929\nEpoch 180/200\n2278/2278 [==============================] - 1s 509us/step - loss: 0.1017 - acc: 0.9745 - val_loss: 2.5299 - val_acc: 0.6890\nEpoch 181/200\n2278/2278 [==============================] - 1s 486us/step - loss: 0.1006 - acc: 0.9772 - val_loss: 2.6438 - val_acc: 0.6732\nEpoch 182/200\n2278/2278 [==============================] - 1s 481us/step - loss: 0.1004 - acc: 0.9759 - val_loss: 2.5942 - val_acc: 0.6850\nEpoch 183/200\n2278/2278 [==============================] - 1s 479us/step - loss: 0.1009 - acc: 0.9781 - val_loss: 2.5874 - val_acc: 0.6929\nEpoch 184/200\n2278/2278 [==============================] - 1s 482us/step - loss: 0.0991 - acc: 0.9763 - val_loss: 2.5528 - val_acc: 0.6929\nEpoch 185/200\n2278/2278 [==============================] - 1s 484us/step - loss: 0.0996 - acc: 0.9776 - val_loss: 2.5768 - val_acc: 0.6929\nEpoch 186/200\n2278/2278 [==============================] - 1s 490us/step - loss: 0.0971 - acc: 0.9785 - val_loss: 2.6531 - val_acc: 0.6772\nEpoch 187/200\n2278/2278 [==============================] - 1s 486us/step - loss: 0.0976 - acc: 0.9767 - val_loss: 2.6376 - val_acc: 0.6772\nEpoch 188/200\n2278/2278 [==============================] - 1s 483us/step - loss: 0.0967 - acc: 0.9763 - val_loss: 2.6612 - val_acc: 0.6772\nEpoch 189/200\n2278/2278 [==============================] - 1s 513us/step - loss: 0.0975 - acc: 0.9776 - val_loss: 2.6535 - val_acc: 0.6772\nEpoch 190/200\n2278/2278 [==============================] - 1s 483us/step - loss: 0.0947 - acc: 0.9754 - val_loss: 2.7138 - val_acc: 0.6772\nEpoch 191/200\n2278/2278 [==============================] - 1s 488us/step - loss: 0.0952 - acc: 0.9781 - val_loss: 2.6778 - val_acc: 0.6890\nEpoch 192/200\n2278/2278 [==============================] - 1s 486us/step - loss: 0.0942 - acc: 0.9781 - val_loss: 2.7352 - val_acc: 0.6850\nEpoch 193/200\n2278/2278 [==============================] - 1s 483us/step - loss: 0.0940 - acc: 0.9781 - val_loss: 2.7067 - val_acc: 0.6850\nEpoch 194/200\n2278/2278 [==============================] - 1s 516us/step - loss: 0.0912 - acc: 0.9781 - val_loss: 2.7190 - val_acc: 0.6811\nEpoch 195/200\n2278/2278 [==============================] - 1s 486us/step - loss: 0.0921 - acc: 0.9781 - val_loss: 2.7502 - val_acc: 0.6732\nEpoch 196/200\n2278/2278 [==============================] - 1s 484us/step - loss: 0.0912 - acc: 0.9789 - val_loss: 2.8029 - val_acc: 0.6732\nEpoch 197/200\n2278/2278 [==============================] - 1s 484us/step - loss: 0.0915 - acc: 0.9772 - val_loss: 2.7967 - val_acc: 0.6811\nEpoch 198/200\n2278/2278 [==============================] - 1s 511us/step - loss: 0.0895 - acc: 0.9794 - val_loss: 2.8723 - val_acc: 0.6811\nEpoch 199/200\n2278/2278 [==============================] - 1s 482us/step - loss: 0.0907 - acc: 0.9794 - val_loss: 2.8853 - val_acc: 0.6732\nEpoch 200/200\n2278/2278 [==============================] - 1s 485us/step - loss: 0.0885 - acc: 0.9789 - val_loss: 2.8120 - val_acc: 0.6732\n2532/2532 [==============================] - 0s 35us/step\n\nacc: 94.98%\n<<<<<<<< ML MODEL CREATED AND SAVED >>>>>>>>>>>\n\n\nStarting file transfer for model/nlc_keras_model.h5 to bucket: myml-donotdelete-pr-zhsoop3fasxh7h\n\nTransfer for model/nlc_keras_model.h5 Complete!\n\n"
                }
            ], 
            "source": "reset_all()"
        }, 
        {
            "execution_count": 59, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "{'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com', 'IBM_AUTH_ENDPOINT': 'https://iam.ng.bluemix.net/oidc/token', 'IBM_API_KEY_ID': 'jDLQvkwwo3h77B5MWgqOTUq25D94Xr6CGrb_6dYmVcj-'}\n{'FILE': 'raw_car_dashboard_ml.csv', 'BUCKET': 'myml-donotdelete-pr-zhsoop3fasxh7h'}\n2532 documents\n26 classes ['about_VA', 'capabilites', 'capabilities', 'compound_questions', 'decision_replies', 'goodbyes', 'greetings', 'improving_system', 'information_request', 'interface_interactions', 'interface_issues', 'locate_amenity', 'navigation', 'negative_reaction', 'not_specified', 'out_of_scope', 'phone', 'positive_reaction', 'selections', 'system_reliance', 'traffic_update', 'turn_down', 'turn_off', 'turn_on', 'turn_up', 'weather']\n"
                }
            ], 
            "source": "reset_for_classification()"
        }, 
        {
            "execution_count": 86, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "<<< ML Model Already Exists >>>>>\n"
                }, 
                {
                    "execution_count": 86, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[('locate_amenity', 1.0)]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "classify('Im searching for a school')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# connectSocketIO()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!pip install wget --upgrade\n!pip install watson-machine-learning-client --upgrade"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from watson_machine_learning_client import WatsonMachineLearningAPIClient"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "client = WatsonMachineLearningAPIClient(wml_credentials)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "instance_details = client.service_instance.get_details()\n\nprint(json.dumps(instance_details, indent=2))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model_props = {client.repository.ModelMetaNames.AUTHOR_NAME: \"Gurvinder Singh\", \n               client.repository.ModelMetaNames.AUTHOR_EMAIL: \"gurvsin3@in.ibm.com\", \n               client.repository.ModelMetaNames.NAME: \"My Custom NLC With Keras\",\n               client.repository.ModelMetaNames.RUNTIME_NAME: \"python\",\n               client.repository.ModelMetaNames.RUNTIME_VERSION: \"3.5\"}\npublished_model = client.repository.store_model(model=model, meta_props=model_props)"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}