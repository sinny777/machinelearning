{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "#### Useful Links: \n  * [Watson Nao Robot Notebook](https://github.com/IBM/watson-nao-robot/blob/master/Notebook/Robo_Notebook.ipynb)\n  * [Watson Document Co-Relation](https://github.com/IBM/watson-document-co-relation)\n    ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "1.9.0\nRequirement already up-to-date: ibm-cos-sdk in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages\nRequirement not upgraded as not directly required: ibm-cos-sdk-s3transfer==2.*,>=2.0.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk)\nRequirement not upgraded as not directly required: ibm-cos-sdk-core==2.*,>=2.0.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk)\nRequirement not upgraded as not directly required: python-dateutil<3.0.0,>=2.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk)\nRequirement not upgraded as not directly required: docutils>=0.10 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk)\nRequirement not upgraded as not directly required: jmespath<1.0.0,>=0.7.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk)\nRequirement not upgraded as not directly required: six>=1.5 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from python-dateutil<3.0.0,>=2.1->ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk)\nRequirement not upgraded as not directly required: tflearn in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages\nRequirement not upgraded as not directly required: six in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from tflearn)\nRequirement not upgraded as not directly required: numpy in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from tflearn)\nRequirement not upgraded as not directly required: Pillow in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from tflearn)\nRequirement not upgraded as not directly required: olefile in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from Pillow->tflearn)\nRequirement already up-to-date: nltk in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages\nRequirement not upgraded as not directly required: six in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from nltk)\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/dsxuser/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /home/dsxuser/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/dsxuser/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package maxent_ne_chunker to\n[nltk_data]     /home/dsxuser/nltk_data...\n[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n[nltk_data] Downloading package words to /home/dsxuser/nltk_data...\n[nltk_data]   Package words is already up-to-date!\nRequirement already up-to-date: socketIO_client_nexus in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages\nRequirement not upgraded as not directly required: requests>=2.7.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from socketIO_client_nexus)\nRequirement not upgraded as not directly required: websocket-client in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from socketIO_client_nexus)\nRequirement not upgraded as not directly required: six in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from socketIO_client_nexus)\nRequirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests>=2.7.0->socketIO_client_nexus)\nRequirement not upgraded as not directly required: idna<2.7,>=2.5 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests>=2.7.0->socketIO_client_nexus)\nRequirement not upgraded as not directly required: urllib3<1.23,>=1.21.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests>=2.7.0->socketIO_client_nexus)\nRequirement not upgraded as not directly required: certifi>=2017.4.17 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests>=2.7.0->socketIO_client_nexus)\n"
                }
            ], 
            "source": "# INSTALL DEPENDENCIES\nimport tensorflow as tf\nif(tf.__version__ == '1.9.0'):\n    print(tf.__version__)\nelse:\n    !pip install --upgrade tensorflow\n    print(tf.__version__)\n\n!pip install -U ibm-cos-sdk\n!pip install tflearn\n!pip install --upgrade nltk\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('maxent_ne_chunker')\nnltk.download('words')\n\n!pip install -U socketIO_client_nexus\n  \nimport pandas as pd\nimport numpy as np\nimport random\n\nimport os.path\nfrom os import path\n\nfrom io import  StringIO\nimport requests\nimport json\nfrom datetime import datetime\nimport time\n\n# things we need for NLP\nimport nltk\nfrom nltk.cluster.util import cosine_distance\nfrom nltk import word_tokenize,sent_tokenize,ne_chunk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.lancaster import LancasterStemmer\nstemmer = LancasterStemmer()\n\nimport sys\nimport types\nfrom botocore.client import Config\nimport ibm_boto3"
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "REMOTE = True"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n# You might want to remove those credentials before you share your notebook.\ndef update_configuration(conf):\n    global config\n    config = conf\n    print(config[\"cos_credentials\"])\n    print(config[\"cos_data\"])\n"
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def multi_part_upload(bucket_name, item_name, file_path):\n    try:\n        print(\"Starting file transfer for {0} to bucket: {1}\\n\".format(item_name, bucket_name))\n        cos = ibm_boto3.resource(service_name='s3',\n            ibm_api_key_id=config[\"cos_credentials\"]['IBM_API_KEY_ID'],\n            ibm_auth_endpoint=config[\"cos_credentials\"]['IBM_AUTH_ENDPOINT'],\n            config=Config(signature_version='oauth'),\n            endpoint_url=config[\"cos_credentials\"]['ENDPOINT'])\n        # set 5 MB chunks\n        part_size = 1024 * 1024 * 5\n\n        # set threadhold to 15 MB\n        file_threshold = 1024 * 1024 * 15\n\n        # set the transfer threshold and chunk size\n        transfer_config = ibm_boto3.s3.transfer.TransferConfig(\n            multipart_threshold=file_threshold,\n            multipart_chunksize=part_size\n        )\n\n        # the upload_fileobj method will automatically execute a multi-part upload \n        # in 5 MB chunks for all files over 15 MB\n        with open(file_path, \"rb\") as file_data:\n            cos.Object(bucket_name, item_name).upload_fileobj(\n                Fileobj=file_data,\n                Config=transfer_config\n            )\n\n        print(\"Transfer for {0} Complete!\\n\".format(item_name))\n    except Exception as e:\n        print(\"Unable to complete multi-part upload: {0}\".format(e))\n"
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def get_object_cos(bucket_name, item_name, path_to_download):\n    try:\n        print(\"Fetching file {0} from bucket: {1}\\n\".format(item_name, bucket_name))\n        cos = ibm_boto3.resource(service_name='s3',\n            ibm_api_key_id=config[\"cos_credentials\"]['IBM_API_KEY_ID'],\n            ibm_auth_endpoint=config[\"cos_credentials\"]['IBM_AUTH_ENDPOINT'],\n            config=Config(signature_version='oauth'),\n            endpoint_url=config[\"cos_credentials\"]['ENDPOINT'])\n        \n        cos.Object(bucket_name, item_name).download_file(path_to_download)\n\n        print(\"Download for {0} Complete!\\n\".format(item_name))\n    except Exception as e:\n        print(\"Unable to download file: {0}\".format(e))\n"
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# LOAD DATA\ndef load_data():\n    global df\n    global cos\n    def __iter__(self): return 0\n\n    # The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n    cos = ibm_boto3.client(service_name='s3',\n        ibm_api_key_id=config[\"cos_credentials\"]['IBM_API_KEY_ID'],\n        ibm_auth_endpoint=config[\"cos_credentials\"]['IBM_AUTH_ENDPOINT'],\n        config=Config(signature_version='oauth'),\n        endpoint_url=config[\"cos_credentials\"]['ENDPOINT'])\n\n    body = cos.get_object(Bucket=config[\"cos_data\"]['BUCKET'],Key=config[\"cos_data\"]['FILE'])['Body']\n    # add missing __iter__ method, so pandas accepts body as file-like object\n    if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\n    df = pd.read_csv(body)\n    df.head()   \n"
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def prepare_documents():\n    global classes\n    global documents\n    global words\n    classes = []\n    documents = []\n    words = []\n    ignore_words = ['?']    \n    \n    # loop through each sentence in our intents patterns\n    for i in range(len(df)):\n        # tokenize each word in the sentence\n        w = nltk.word_tokenize(df[\"utterances\"][i])\n        # add to our words list\n        words.extend(w)\n        # add to documents in our corpus\n        documents.append((w, df[\"intent\"][i]))\n        # add to our classes list\n        if df[\"intent\"][i] not in classes:\n            classes.append(df[\"intent\"][i])\n\n    # stem and lower each word and remove duplicates\n    words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n    words = sorted(list(set(words)))\n\n    # remove duplicates\n    classes = sorted(list(set(classes)))\n\n    print (len(documents), \"documents\")\n    print (len(classes), \"classes\", classes)\n    # print (len(words), \"unique stemmed words\", words)\n"
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# create our training data\ndef prepare_for_training():\n    training = []\n    output = []\n    global train_x\n    global train_y\n    # create an empty array for our output\n    output_empty = [0] * len(classes)\n    # training set, bag of words for each sentence\n    for doc in documents:\n        # initialize our bag of words\n        bag = []\n        # list of tokenized words for the pattern\n        pattern_words = doc[0]\n        # stem each word\n        pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n        # create our bag of words array\n        for w in words:\n            bag.append(1) if w in pattern_words else bag.append(0)\n\n        # output is a '0' for each tag and '1' for current tag\n        output_row = list(output_empty)\n        output_row[classes.index(doc[1])] = 1\n\n        training.append([bag, output_row])\n        \n    # shuffle our features and turn into np.array\n    random.shuffle(training)\n    training = np.array(training)\n    \n    # create train and test lists\n    train_x = list(training[:,0])\n    train_y = list(training[:,1])\n"
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "Using TensorFlow backend.\n"
                }
            ], 
            "source": "from keras.models import Sequential\nfrom keras.layers import Dense, Input, concatenate, Activation\nfrom keras.layers.pooling import GlobalMaxPooling1D, MaxPooling1D\nfrom keras.layers.core import Dropout\nfrom keras import backend as K\n\n# CREATE ML MODEL\ndef create_model():\n    K.clear_session()\n    # tf.global_variables_initializer()\n    tf.reset_default_graph()\n    global model\n    model = Sequential()\n    # model.add(Dense(output_dim=8,init ='uniform',activation='relu', input_dim=len(train_x[0])))\n    model.add(Dense(8, input_shape=(np.asarray(train_x[0]).shape)))\n    # model.add(Dense(8, input_shape=(len(train_x[0]), )))\n    # model.add(Dense(8, activation='relu', input_dim=(len(train_x))))\n    # model.add(Activation('relu'))\n    # model.add(Dropout(0.3))\n    model.add(Dense(8))\n    # model.add(Activation('relu'))\n    # model.add(Dropout(0.3))\n    model.add(Dense(8))\n    # model.add(Activation('relu'))\n    # model.add(Dropout(0.3))\n    model.add(Dense(np.asarray(train_y[0]).shape[0], init ='uniform', activation='softmax'))\n    model.summary()\n    \n    tbCallBack = tf.keras.callbacks.TensorBoard(log_dir='keras_logs', write_graph=True)\n    \n    # model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model.fit(np.asarray(train_x), np.asarray(train_y), epochs=150, batch_size=16,  verbose=1, validation_split=0.1, callbacks=[tbCallBack])\n    scores = model.evaluate(np.asarray(train_x), np.asarray(train_y))\n    print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n    model.save('nlc_keras_model.h5')\n    print(\"<<<<<<<< ML MODEL CREATED AND SAVED >>>>>>>>>>>\\n\\n\")"
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def save_model_COS():\n    if(REMOTE):\n        multi_part_upload(config[\"cos_data\"]['BUCKET'], \"model/nlc_keras_model.h5\", \"nlc_keras_model.h5\")\n    "
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def reset_all():\n    update_configuration(conf)\n    load_data()\n    prepare_documents()\n    prepare_for_training()\n    create_model()\n    save_model_COS()\n"
        }, 
        {
            "source": "# Code to Classify text using the ML Model created", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def fetch_ml_model_cos():\n    if(path.exists('nlc_keras_model.h5') == False):\n        get_object_cos(config[\"cos_data\"]['BUCKET'], \"model/nlc_keras_model.h5\", \"nlc_keras_model.h5\")    \n"
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from keras.models import load_model\ndef load_ml_model():\n    global model\n    try:\n        model\n    except NameError:\n        print(\"<<< ML Model Needs to be loaded >>>>>\")\n        # load our saved model\n        fetch_ml_model_cos()\n        model = load_model('nlc_keras_model.h5')        \n    else:\n        print(\"<<< ML Model Already Exists >>>>>\")        \n"
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def clean_up_sentence(sentence):\n    # tokenize the pattern\n    sentence_words = nltk.word_tokenize(sentence)\n    # stem each word\n    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n    return sentence_words\n\n# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\ndef bow(sentence, words, show_details=False):\n    # tokenize the pattern\n    sentence_words = clean_up_sentence(sentence)\n    # bag of words\n    bag = [0]*len(words)  \n    for s in sentence_words:\n        for i,w in enumerate(words):\n            if w == s: \n                bag[i] = 1\n                if show_details:\n                    print (\"found in bag: %s\" % w)\n\n    return(np.array(bag))"
        }, 
        {
            "execution_count": 16, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# create a data structure to hold user context\ncontext = {}\n\nERROR_THRESHOLD = 0.25\ndef classify(sentence):\n    # generate probabilities from the model\n    load_ml_model()\n    to_predict = bow(sentence, words)\n    print(type(to_predict))\n    if (to_predict.ndim == 1):\n        to_predict = np.array([to_predict])\n    \n    results = model.predict([to_predict])[0]\n    # filter out predictions below a threshold\n    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n    # sort by strength of probability\n    results.sort(key=lambda x: x[1], reverse=True)\n    return_list = []\n    for r in results:\n        return_list.append((classes[r[0]], r[1]))\n    # return tuple of intent and probability\n    return return_list"
        }, 
        {
            "execution_count": 17, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def reset_for_classification():\n    update_configuration(conf)\n    load_data()\n    prepare_documents()\n    prepare_for_training()"
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from socketIO_client_nexus import SocketIO, BaseNamespace, LoggingNamespace\n\ndef on_connect():\n    print('on_connect')\n\ndef on_disconnect():\n    print('on_disconnect')\n\ndef on_reconnect():\n    print('on_reconnect')\n\ndef on_response(*message):\n    msg = json.loads(json.dumps(message))\n    print(type(msg))\n    print('\\n\\non_response: >> ', msg[0])\n    command = msg[0][\"command\"]\n    params = msg[0][\"params\"]\n    if command == \"reset_all\":\n        reset_all()\n    elif command == \"classify\":\n        results = classify(params[\"text\"])\n        print(results)\n    else:\n        print(\"Command not recognized....\")\n\ndef connectSocketIO():\n#     SocketIO('https://localhost', verify=False)\n    with SocketIO('https://my-watson-assistant-api.mybluemix.net', verify=False) as socketIO:\n        # with SocketIO('localhost', verify=False) as socketIO:\n        socketIO.on('connect', on_connect)\n        socketIO.on('disconnect', on_disconnect)\n        socketIO.on('reconnect', on_reconnect)\n        socketIO.on('/ml', on_response)\n        socketIO.wait()\n"
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "{'IBM_AUTH_ENDPOINT': 'https://iam.ng.bluemix.net/oidc/token', 'IBM_API_KEY_ID': 'jDLQvkwwo3h77B5MWgqOTUq25D94Xr6CGrb_6dYmVcj-', 'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com'}\n{'FILE': 'raw_car_dashboard_ml.csv', 'BUCKET': 'myml-donotdelete-pr-zhsoop3fasxh7h'}\n2532 documents\n26 classes ['about_VA', 'capabilites', 'capabilities', 'compound_questions', 'decision_replies', 'goodbyes', 'greetings', 'improving_system', 'information_request', 'interface_interactions', 'interface_issues', 'locate_amenity', 'navigation', 'negative_reaction', 'not_specified', 'out_of_scope', 'phone', 'positive_reaction', 'selections', 'system_reliance', 'traffic_update', 'turn_down', 'turn_off', 'turn_on', 'turn_up', 'weather']\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/__main__.py:26: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(26, kernel_initializer=\"uniform\", activation=\"softmax\")`\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 8)                 10656     \n_________________________________________________________________\ndense_2 (Dense)              (None, 8)                 72        \n_________________________________________________________________\ndense_3 (Dense)              (None, 8)                 72        \n_________________________________________________________________\ndense_4 (Dense)              (None, 26)                234       \n=================================================================\nTotal params: 11,034\nTrainable params: 11,034\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 2278 samples, validate on 254 samples\nEpoch 1/150\n2278/2278 [==============================] - 1s 532us/step - loss: 2.7325 - acc: 0.2897 - val_loss: 1.9875 - val_acc: 0.3150\nEpoch 2/150\n2278/2278 [==============================] - 1s 384us/step - loss: 2.0078 - acc: 0.2994 - val_loss: 1.7469 - val_acc: 0.3543\nEpoch 3/150\n2278/2278 [==============================] - 1s 362us/step - loss: 1.7656 - acc: 0.4772 - val_loss: 1.4387 - val_acc: 0.6181\nEpoch 4/150\n2278/2278 [==============================] - 1s 368us/step - loss: 1.3919 - acc: 0.5944 - val_loss: 1.1737 - val_acc: 0.6417\nEpoch 5/150\n2278/2278 [==============================] - 1s 385us/step - loss: 1.1945 - acc: 0.6124 - val_loss: 1.0859 - val_acc: 0.6772\nEpoch 6/150\n2278/2278 [==============================] - 1s 390us/step - loss: 1.0525 - acc: 0.6673 - val_loss: 1.0204 - val_acc: 0.6890\nEpoch 7/150\n2278/2278 [==============================] - 1s 365us/step - loss: 0.9138 - acc: 0.7261 - val_loss: 0.9641 - val_acc: 0.7126\nEpoch 8/150\n2278/2278 [==============================] - 1s 391us/step - loss: 0.7985 - acc: 0.7594 - val_loss: 0.9225 - val_acc: 0.7480\nEpoch 9/150\n2278/2278 [==============================] - 1s 385us/step - loss: 0.7148 - acc: 0.7783 - val_loss: 0.9036 - val_acc: 0.7520\nEpoch 10/150\n2278/2278 [==============================] - 1s 386us/step - loss: 0.6497 - acc: 0.7937 - val_loss: 0.8829 - val_acc: 0.7638\nEpoch 11/150\n2278/2278 [==============================] - 1s 363us/step - loss: 0.5937 - acc: 0.8200 - val_loss: 0.8823 - val_acc: 0.7598\nEpoch 12/150\n2278/2278 [==============================] - 1s 393us/step - loss: 0.5439 - acc: 0.8367 - val_loss: 0.8785 - val_acc: 0.7717\nEpoch 13/150\n2278/2278 [==============================] - 1s 384us/step - loss: 0.5062 - acc: 0.8468 - val_loss: 0.8728 - val_acc: 0.7717\nEpoch 14/150\n2278/2278 [==============================] - 1s 364us/step - loss: 0.4672 - acc: 0.8582 - val_loss: 0.8745 - val_acc: 0.7717\nEpoch 15/150\n2278/2278 [==============================] - 1s 388us/step - loss: 0.4349 - acc: 0.8740 - val_loss: 0.8954 - val_acc: 0.7835\nEpoch 16/150\n2278/2278 [==============================] - 1s 390us/step - loss: 0.4076 - acc: 0.8854 - val_loss: 0.8860 - val_acc: 0.7677\nEpoch 17/150\n2278/2278 [==============================] - 1s 391us/step - loss: 0.3828 - acc: 0.8938 - val_loss: 0.9100 - val_acc: 0.7756\nEpoch 18/150\n2278/2278 [==============================] - 1s 359us/step - loss: 0.3598 - acc: 0.8995 - val_loss: 0.9049 - val_acc: 0.7795\nEpoch 19/150\n2278/2278 [==============================] - 1s 392us/step - loss: 0.3427 - acc: 0.9069 - val_loss: 0.9128 - val_acc: 0.7717\nEpoch 20/150\n2278/2278 [==============================] - 1s 393us/step - loss: 0.3250 - acc: 0.9109 - val_loss: 0.9486 - val_acc: 0.7717\nEpoch 21/150\n2278/2278 [==============================] - 1s 391us/step - loss: 0.3096 - acc: 0.9135 - val_loss: 0.9632 - val_acc: 0.7677\nEpoch 22/150\n2278/2278 [==============================] - 1s 393us/step - loss: 0.2986 - acc: 0.9157 - val_loss: 0.9843 - val_acc: 0.7717\nEpoch 23/150\n2278/2278 [==============================] - 1s 385us/step - loss: 0.2827 - acc: 0.9175 - val_loss: 1.0375 - val_acc: 0.7756\nEpoch 24/150\n2278/2278 [==============================] - 1s 357us/step - loss: 0.2749 - acc: 0.9197 - val_loss: 1.0462 - val_acc: 0.7756\nEpoch 25/150\n2278/2278 [==============================] - 1s 388us/step - loss: 0.2643 - acc: 0.9223 - val_loss: 1.0552 - val_acc: 0.7717\nEpoch 26/150\n2278/2278 [==============================] - 1s 396us/step - loss: 0.2551 - acc: 0.9232 - val_loss: 1.0534 - val_acc: 0.7795\nEpoch 27/150\n2278/2278 [==============================] - 1s 365us/step - loss: 0.2467 - acc: 0.9267 - val_loss: 1.1056 - val_acc: 0.7717\nEpoch 28/150\n2278/2278 [==============================] - 1s 377us/step - loss: 0.2384 - acc: 0.9271 - val_loss: 1.1267 - val_acc: 0.7717\nEpoch 29/150\n2278/2278 [==============================] - 1s 362us/step - loss: 0.2322 - acc: 0.9298 - val_loss: 1.1420 - val_acc: 0.7835\nEpoch 30/150\n2278/2278 [==============================] - 1s 391us/step - loss: 0.2237 - acc: 0.9284 - val_loss: 1.1536 - val_acc: 0.7795\nEpoch 31/150\n2278/2278 [==============================] - 1s 383us/step - loss: 0.2175 - acc: 0.9355 - val_loss: 1.1895 - val_acc: 0.7717\nEpoch 32/150\n2278/2278 [==============================] - 1s 362us/step - loss: 0.2142 - acc: 0.9368 - val_loss: 1.2314 - val_acc: 0.7795\nEpoch 33/150\n2278/2278 [==============================] - 1s 391us/step - loss: 0.2104 - acc: 0.9324 - val_loss: 1.2252 - val_acc: 0.7677\nEpoch 34/150\n2278/2278 [==============================] - 1s 385us/step - loss: 0.2035 - acc: 0.9355 - val_loss: 1.2449 - val_acc: 0.7756\nEpoch 35/150\n2278/2278 [==============================] - 1s 392us/step - loss: 0.1978 - acc: 0.9434 - val_loss: 1.2942 - val_acc: 0.7717\nEpoch 36/150\n2278/2278 [==============================] - 1s 391us/step - loss: 0.1935 - acc: 0.9416 - val_loss: 1.2829 - val_acc: 0.7835\nEpoch 37/150\n2278/2278 [==============================] - 1s 364us/step - loss: 0.1944 - acc: 0.9346 - val_loss: 1.3319 - val_acc: 0.7717\nEpoch 38/150\n2278/2278 [==============================] - 1s 395us/step - loss: 0.1914 - acc: 0.9434 - val_loss: 1.3208 - val_acc: 0.7795\nEpoch 39/150\n2278/2278 [==============================] - 1s 386us/step - loss: 0.1830 - acc: 0.9447 - val_loss: 1.3258 - val_acc: 0.7756\nEpoch 40/150\n2278/2278 [==============================] - 1s 386us/step - loss: 0.1770 - acc: 0.9412 - val_loss: 1.3567 - val_acc: 0.7677\nEpoch 41/150\n2278/2278 [==============================] - 1s 361us/step - loss: 0.1723 - acc: 0.9438 - val_loss: 1.3775 - val_acc: 0.7677\nEpoch 42/150\n2278/2278 [==============================] - 1s 385us/step - loss: 0.1696 - acc: 0.9482 - val_loss: 1.4023 - val_acc: 0.7835\nEpoch 43/150\n2278/2278 [==============================] - 1s 354us/step - loss: 0.1671 - acc: 0.9447 - val_loss: 1.4113 - val_acc: 0.7717\nEpoch 44/150\n2278/2278 [==============================] - 1s 386us/step - loss: 0.1632 - acc: 0.9486 - val_loss: 1.3994 - val_acc: 0.7520\nEpoch 45/150\n2278/2278 [==============================] - 1s 363us/step - loss: 0.1632 - acc: 0.9451 - val_loss: 1.4490 - val_acc: 0.7677\nEpoch 46/150\n2278/2278 [==============================] - 1s 389us/step - loss: 0.1547 - acc: 0.9500 - val_loss: 1.4641 - val_acc: 0.7638\nEpoch 47/150\n2278/2278 [==============================] - 1s 391us/step - loss: 0.1554 - acc: 0.9500 - val_loss: 1.5099 - val_acc: 0.7638\nEpoch 48/150\n2278/2278 [==============================] - 1s 356us/step - loss: 0.1537 - acc: 0.9513 - val_loss: 1.5242 - val_acc: 0.7638\nEpoch 49/150\n2278/2278 [==============================] - 1s 388us/step - loss: 0.1488 - acc: 0.9513 - val_loss: 1.5298 - val_acc: 0.7717\nEpoch 50/150\n2278/2278 [==============================] - 1s 393us/step - loss: 0.1493 - acc: 0.9522 - val_loss: 1.5512 - val_acc: 0.7598\nEpoch 51/150\n2278/2278 [==============================] - 1s 363us/step - loss: 0.1439 - acc: 0.9543 - val_loss: 1.5510 - val_acc: 0.7598\nEpoch 52/150\n2278/2278 [==============================] - 1s 395us/step - loss: 0.1410 - acc: 0.9561 - val_loss: 1.5432 - val_acc: 0.7677\nEpoch 53/150\n2278/2278 [==============================] - 1s 361us/step - loss: 0.1397 - acc: 0.9517 - val_loss: 1.5677 - val_acc: 0.7638\nEpoch 54/150\n2278/2278 [==============================] - 1s 391us/step - loss: 0.1357 - acc: 0.9592 - val_loss: 1.5908 - val_acc: 0.7756\nEpoch 55/150\n2278/2278 [==============================] - 1s 389us/step - loss: 0.1327 - acc: 0.9605 - val_loss: 1.6190 - val_acc: 0.7559\nEpoch 56/150\n2278/2278 [==============================] - 1s 366us/step - loss: 0.1295 - acc: 0.9587 - val_loss: 1.6262 - val_acc: 0.7677\nEpoch 57/150\n2278/2278 [==============================] - 1s 392us/step - loss: 0.1264 - acc: 0.9605 - val_loss: 1.6363 - val_acc: 0.7677\nEpoch 58/150\n2278/2278 [==============================] - 1s 393us/step - loss: 0.1236 - acc: 0.9601 - val_loss: 1.6448 - val_acc: 0.7598\nEpoch 59/150\n2278/2278 [==============================] - 1s 389us/step - loss: 0.1221 - acc: 0.9622 - val_loss: 1.6348 - val_acc: 0.7677\nEpoch 60/150\n2278/2278 [==============================] - 1s 398us/step - loss: 0.1187 - acc: 0.9622 - val_loss: 1.6693 - val_acc: 0.7638\nEpoch 61/150\n2278/2278 [==============================] - 1s 353us/step - loss: 0.1158 - acc: 0.9640 - val_loss: 1.6765 - val_acc: 0.7717\nEpoch 62/150\n2278/2278 [==============================] - 1s 378us/step - loss: 0.1186 - acc: 0.9622 - val_loss: 1.6678 - val_acc: 0.7559\nEpoch 63/150\n2278/2278 [==============================] - 1s 360us/step - loss: 0.1118 - acc: 0.9666 - val_loss: 1.7075 - val_acc: 0.7717\nEpoch 64/150\n2278/2278 [==============================] - 1s 354us/step - loss: 0.1091 - acc: 0.9671 - val_loss: 1.7221 - val_acc: 0.7598\nEpoch 65/150\n2278/2278 [==============================] - 1s 353us/step - loss: 0.1048 - acc: 0.9653 - val_loss: 1.7439 - val_acc: 0.7559\nEpoch 66/150\n2278/2278 [==============================] - 1s 383us/step - loss: 0.1041 - acc: 0.9701 - val_loss: 1.7444 - val_acc: 0.7717\nEpoch 67/150\n2278/2278 [==============================] - 1s 352us/step - loss: 0.1029 - acc: 0.9715 - val_loss: 1.7225 - val_acc: 0.7638\nEpoch 68/150\n2278/2278 [==============================] - 1s 354us/step - loss: 0.1006 - acc: 0.9701 - val_loss: 1.7538 - val_acc: 0.7677\nEpoch 69/150\n2278/2278 [==============================] - 1s 352us/step - loss: 0.0975 - acc: 0.9693 - val_loss: 1.7592 - val_acc: 0.7559\nEpoch 70/150\n2278/2278 [==============================] - 1s 352us/step - loss: 0.0984 - acc: 0.9680 - val_loss: 1.7677 - val_acc: 0.7638\nEpoch 71/150\n2278/2278 [==============================] - 1s 380us/step - loss: 0.0945 - acc: 0.9701 - val_loss: 1.7758 - val_acc: 0.7598\nEpoch 72/150\n2278/2278 [==============================] - 1s 357us/step - loss: 0.0896 - acc: 0.9732 - val_loss: 1.7502 - val_acc: 0.7717\nEpoch 73/150\n2278/2278 [==============================] - 1s 354us/step - loss: 0.0863 - acc: 0.9737 - val_loss: 1.7812 - val_acc: 0.7559\nEpoch 74/150\n2278/2278 [==============================] - 1s 381us/step - loss: 0.0839 - acc: 0.9745 - val_loss: 1.8159 - val_acc: 0.7638\nEpoch 75/150\n2278/2278 [==============================] - 1s 360us/step - loss: 0.0851 - acc: 0.9759 - val_loss: 1.8018 - val_acc: 0.7677\nEpoch 76/150\n2278/2278 [==============================] - 1s 356us/step - loss: 0.0810 - acc: 0.9759 - val_loss: 1.7978 - val_acc: 0.7677\nEpoch 77/150\n2278/2278 [==============================] - 1s 383us/step - loss: 0.0767 - acc: 0.9772 - val_loss: 1.8194 - val_acc: 0.7598\nEpoch 78/150\n2278/2278 [==============================] - 1s 353us/step - loss: 0.0765 - acc: 0.9785 - val_loss: 1.8291 - val_acc: 0.7677\nEpoch 79/150\n2278/2278 [==============================] - 1s 355us/step - loss: 0.0755 - acc: 0.9781 - val_loss: 1.8480 - val_acc: 0.7638\nEpoch 80/150\n2278/2278 [==============================] - 1s 385us/step - loss: 0.0757 - acc: 0.9763 - val_loss: 1.8661 - val_acc: 0.7677\nEpoch 81/150\n2278/2278 [==============================] - 1s 357us/step - loss: 0.0697 - acc: 0.9816 - val_loss: 1.8329 - val_acc: 0.7717\nEpoch 82/150\n2278/2278 [==============================] - 1s 355us/step - loss: 0.0683 - acc: 0.9785 - val_loss: 1.8350 - val_acc: 0.7717\nEpoch 83/150\n2278/2278 [==============================] - 1s 384us/step - loss: 0.0662 - acc: 0.9820 - val_loss: 1.8592 - val_acc: 0.7717\nEpoch 84/150\n2278/2278 [==============================] - 1s 356us/step - loss: 0.0652 - acc: 0.9811 - val_loss: 1.8423 - val_acc: 0.7638\nEpoch 85/150\n2278/2278 [==============================] - 1s 356us/step - loss: 0.0647 - acc: 0.9833 - val_loss: 1.8708 - val_acc: 0.7598\nEpoch 86/150\n2278/2278 [==============================] - 1s 384us/step - loss: 0.0623 - acc: 0.9816 - val_loss: 1.9088 - val_acc: 0.7756\nEpoch 87/150\n2278/2278 [==============================] - 1s 362us/step - loss: 0.0590 - acc: 0.9833 - val_loss: 1.9255 - val_acc: 0.7717\nEpoch 88/150\n2278/2278 [==============================] - 1s 359us/step - loss: 0.0576 - acc: 0.9846 - val_loss: 1.8794 - val_acc: 0.7756\nEpoch 89/150\n2278/2278 [==============================] - 1s 354us/step - loss: 0.0604 - acc: 0.9824 - val_loss: 1.9246 - val_acc: 0.7795\nEpoch 90/150\n2278/2278 [==============================] - 1s 351us/step - loss: 0.0549 - acc: 0.9855 - val_loss: 1.8961 - val_acc: 0.7756\nEpoch 91/150\n2278/2278 [==============================] - 1s 380us/step - loss: 0.0535 - acc: 0.9855 - val_loss: 1.9125 - val_acc: 0.7795\nEpoch 92/150\n2278/2278 [==============================] - 1s 355us/step - loss: 0.0543 - acc: 0.9855 - val_loss: 1.9274 - val_acc: 0.7717\nEpoch 93/150\n2278/2278 [==============================] - 1s 319us/step - loss: 0.0512 - acc: 0.9860 - val_loss: 1.9389 - val_acc: 0.7638\nEpoch 94/150\n2278/2278 [==============================] - 1s 384us/step - loss: 0.0500 - acc: 0.9873 - val_loss: 1.9808 - val_acc: 0.7717\nEpoch 95/150\n2278/2278 [==============================] - 1s 356us/step - loss: 0.0538 - acc: 0.9873 - val_loss: 1.9768 - val_acc: 0.7717\nEpoch 96/150\n2278/2278 [==============================] - 1s 384us/step - loss: 0.0526 - acc: 0.9820 - val_loss: 2.0334 - val_acc: 0.7717\nEpoch 97/150\n2278/2278 [==============================] - 1s 357us/step - loss: 0.0496 - acc: 0.9833 - val_loss: 1.9937 - val_acc: 0.7677\nEpoch 98/150\n2278/2278 [==============================] - 1s 356us/step - loss: 0.0447 - acc: 0.9881 - val_loss: 2.0080 - val_acc: 0.7756\nEpoch 99/150\n2278/2278 [==============================] - 1s 380us/step - loss: 0.0445 - acc: 0.9864 - val_loss: 2.0180 - val_acc: 0.7717\nEpoch 100/150\n2278/2278 [==============================] - 1s 353us/step - loss: 0.0447 - acc: 0.9877 - val_loss: 1.9888 - val_acc: 0.7677\nEpoch 101/150\n2278/2278 [==============================] - 1s 356us/step - loss: 0.0413 - acc: 0.9881 - val_loss: 2.0393 - val_acc: 0.7835\nEpoch 102/150\n2278/2278 [==============================] - 1s 351us/step - loss: 0.0438 - acc: 0.9864 - val_loss: 2.0278 - val_acc: 0.7717\nEpoch 103/150\n2278/2278 [==============================] - 1s 378us/step - loss: 0.0409 - acc: 0.9864 - val_loss: 2.0587 - val_acc: 0.7795\nEpoch 104/150\n2278/2278 [==============================] - 1s 355us/step - loss: 0.0419 - acc: 0.9886 - val_loss: 2.0561 - val_acc: 0.7717\nEpoch 105/150\n2278/2278 [==============================] - 1s 353us/step - loss: 0.0401 - acc: 0.9873 - val_loss: 2.0446 - val_acc: 0.7756\nEpoch 106/150\n2278/2278 [==============================] - 1s 355us/step - loss: 0.0365 - acc: 0.9903 - val_loss: 2.0852 - val_acc: 0.7638\nEpoch 107/150\n2278/2278 [==============================] - 1s 381us/step - loss: 0.0379 - acc: 0.9886 - val_loss: 2.1028 - val_acc: 0.7874\nEpoch 108/150\n2278/2278 [==============================] - 1s 355us/step - loss: 0.0370 - acc: 0.9899 - val_loss: 2.0537 - val_acc: 0.7756\nEpoch 109/150\n2278/2278 [==============================] - 1s 356us/step - loss: 0.0338 - acc: 0.9895 - val_loss: 2.0547 - val_acc: 0.7874\nEpoch 110/150\n2278/2278 [==============================] - 1s 354us/step - loss: 0.0336 - acc: 0.9886 - val_loss: 2.1215 - val_acc: 0.7953\nEpoch 111/150\n2278/2278 [==============================] - 1s 386us/step - loss: 0.0336 - acc: 0.9890 - val_loss: 2.0888 - val_acc: 0.7756\nEpoch 112/150\n2278/2278 [==============================] - 1s 357us/step - loss: 0.0347 - acc: 0.9899 - val_loss: 2.0784 - val_acc: 0.7795\nEpoch 113/150\n2278/2278 [==============================] - 1s 380us/step - loss: 0.0319 - acc: 0.9899 - val_loss: 2.0978 - val_acc: 0.7874\nEpoch 114/150\n2278/2278 [==============================] - 1s 357us/step - loss: 0.0330 - acc: 0.9903 - val_loss: 2.1106 - val_acc: 0.7913\nEpoch 115/150\n2278/2278 [==============================] - 1s 351us/step - loss: 0.0312 - acc: 0.9895 - val_loss: 2.0940 - val_acc: 0.7874\nEpoch 116/150\n2278/2278 [==============================] - 1s 356us/step - loss: 0.0302 - acc: 0.9908 - val_loss: 2.1145 - val_acc: 0.7795\nEpoch 117/150\n2278/2278 [==============================] - 1s 378us/step - loss: 0.0285 - acc: 0.9903 - val_loss: 2.1046 - val_acc: 0.7874\nEpoch 118/150\n2278/2278 [==============================] - 1s 358us/step - loss: 0.0278 - acc: 0.9917 - val_loss: 2.1143 - val_acc: 0.7913\nEpoch 119/150\n2278/2278 [==============================] - 1s 353us/step - loss: 0.0269 - acc: 0.9921 - val_loss: 2.1439 - val_acc: 0.7835\nEpoch 120/150\n2278/2278 [==============================] - 1s 355us/step - loss: 0.0277 - acc: 0.9912 - val_loss: 2.1236 - val_acc: 0.7874\nEpoch 121/150\n2278/2278 [==============================] - 1s 378us/step - loss: 0.0294 - acc: 0.9912 - val_loss: 2.1737 - val_acc: 0.7953\nEpoch 122/150\n2278/2278 [==============================] - 1s 359us/step - loss: 0.0266 - acc: 0.9921 - val_loss: 2.2157 - val_acc: 0.7835\nEpoch 123/150\n2278/2278 [==============================] - 1s 355us/step - loss: 0.0262 - acc: 0.9921 - val_loss: 2.1559 - val_acc: 0.7835\nEpoch 124/150\n2278/2278 [==============================] - 1s 383us/step - loss: 0.0255 - acc: 0.9903 - val_loss: 2.1923 - val_acc: 0.7874\nEpoch 125/150\n2278/2278 [==============================] - 1s 356us/step - loss: 0.0225 - acc: 0.9934 - val_loss: 2.1555 - val_acc: 0.7835\nEpoch 126/150\n2278/2278 [==============================] - 1s 353us/step - loss: 0.0245 - acc: 0.9921 - val_loss: 2.1746 - val_acc: 0.7835\nEpoch 127/150\n2278/2278 [==============================] - 1s 356us/step - loss: 0.0236 - acc: 0.9917 - val_loss: 2.1704 - val_acc: 0.7913\nEpoch 128/150\n2278/2278 [==============================] - 1s 353us/step - loss: 0.0247 - acc: 0.9921 - val_loss: 2.1579 - val_acc: 0.7953\nEpoch 129/150\n2278/2278 [==============================] - 1s 350us/step - loss: 0.0236 - acc: 0.9925 - val_loss: 2.1548 - val_acc: 0.8031\nEpoch 130/150\n2278/2278 [==============================] - 1s 383us/step - loss: 0.0199 - acc: 0.9947 - val_loss: 2.2067 - val_acc: 0.7953\nEpoch 131/150\n2278/2278 [==============================] - 1s 354us/step - loss: 0.0201 - acc: 0.9925 - val_loss: 2.2570 - val_acc: 0.7913\nEpoch 132/150\n2278/2278 [==============================] - 1s 359us/step - loss: 0.0239 - acc: 0.9921 - val_loss: 2.1659 - val_acc: 0.7992\nEpoch 133/150\n2278/2278 [==============================] - 1s 377us/step - loss: 0.0225 - acc: 0.9917 - val_loss: 2.1684 - val_acc: 0.7835\nEpoch 134/150\n2278/2278 [==============================] - 1s 355us/step - loss: 0.0190 - acc: 0.9939 - val_loss: 2.1948 - val_acc: 0.7913\nEpoch 135/150\n2278/2278 [==============================] - 1s 353us/step - loss: 0.0213 - acc: 0.9921 - val_loss: 2.2285 - val_acc: 0.7913\nEpoch 136/150\n2278/2278 [==============================] - 1s 350us/step - loss: 0.0235 - acc: 0.9925 - val_loss: 2.2626 - val_acc: 0.7835\nEpoch 137/150\n2278/2278 [==============================] - 1s 357us/step - loss: 0.0190 - acc: 0.9943 - val_loss: 2.2543 - val_acc: 0.7795\nEpoch 138/150\n2278/2278 [==============================] - 1s 382us/step - loss: 0.0210 - acc: 0.9930 - val_loss: 2.2191 - val_acc: 0.7835\nEpoch 139/150\n2278/2278 [==============================] - 1s 357us/step - loss: 0.0183 - acc: 0.9939 - val_loss: 2.2345 - val_acc: 0.7913\nEpoch 140/150\n2278/2278 [==============================] - 1s 381us/step - loss: 0.0190 - acc: 0.9934 - val_loss: 2.1977 - val_acc: 0.7913\nEpoch 141/150\n2278/2278 [==============================] - 1s 355us/step - loss: 0.0180 - acc: 0.9921 - val_loss: 2.2398 - val_acc: 0.7913\nEpoch 142/150\n2278/2278 [==============================] - 1s 357us/step - loss: 0.0187 - acc: 0.9930 - val_loss: 2.2140 - val_acc: 0.7835\nEpoch 143/150\n2278/2278 [==============================] - 1s 350us/step - loss: 0.0191 - acc: 0.9925 - val_loss: 2.2552 - val_acc: 0.7913\nEpoch 144/150\n2278/2278 [==============================] - 1s 380us/step - loss: 0.0177 - acc: 0.9934 - val_loss: 2.2466 - val_acc: 0.7874\nEpoch 145/150\n2278/2278 [==============================] - 1s 322us/step - loss: 0.0168 - acc: 0.9939 - val_loss: 2.2309 - val_acc: 0.7835\nEpoch 146/150\n2278/2278 [==============================] - 1s 385us/step - loss: 0.0184 - acc: 0.9943 - val_loss: 2.2312 - val_acc: 0.7913\nEpoch 147/150\n2278/2278 [==============================] - 1s 355us/step - loss: 0.0177 - acc: 0.9939 - val_loss: 2.2769 - val_acc: 0.7795\nEpoch 148/150\n2278/2278 [==============================] - 1s 353us/step - loss: 0.0161 - acc: 0.9947 - val_loss: 2.2287 - val_acc: 0.7874\nEpoch 149/150\n2278/2278 [==============================] - 1s 349us/step - loss: 0.0156 - acc: 0.9943 - val_loss: 2.3021 - val_acc: 0.7874\nEpoch 150/150\n2278/2278 [==============================] - 1s 352us/step - loss: 0.0154 - acc: 0.9943 - val_loss: 2.2616 - val_acc: 0.7835\n2532/2532 [==============================] - 0s 39us/step\n\nacc: 97.47%\n<<<<<<<< ML MODEL CREATED AND SAVED >>>>>>>>>>>\n\n\nStarting file transfer for model/nlc_keras_model.h5 to bucket: myml-donotdelete-pr-zhsoop3fasxh7h\n\nTransfer for model/nlc_keras_model.h5 Complete!\n\n"
                }
            ], 
            "source": "reset_all()"
        }, 
        {
            "execution_count": 21, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "{'IBM_AUTH_ENDPOINT': 'https://iam.ng.bluemix.net/oidc/token', 'IBM_API_KEY_ID': 'jDLQvkwwo3h77B5MWgqOTUq25D94Xr6CGrb_6dYmVcj-', 'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com'}\n{'FILE': 'raw_car_dashboard_ml.csv', 'BUCKET': 'myml-donotdelete-pr-zhsoop3fasxh7h'}\n2532 documents\n26 classes ['about_VA', 'capabilites', 'capabilities', 'compound_questions', 'decision_replies', 'goodbyes', 'greetings', 'improving_system', 'information_request', 'interface_interactions', 'interface_issues', 'locate_amenity', 'navigation', 'negative_reaction', 'not_specified', 'out_of_scope', 'phone', 'positive_reaction', 'selections', 'system_reliance', 'traffic_update', 'turn_down', 'turn_off', 'turn_on', 'turn_up', 'weather']\n"
                }
            ], 
            "source": "reset_for_classification()"
        }, 
        {
            "execution_count": 22, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "<<< ML Model Already Exists >>>>>\n<class 'numpy.ndarray'>\n"
                }, 
                {
                    "execution_count": 22, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[('navigation', 0.91700786)]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "classify('how can I go home')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# connectSocketIO()"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}